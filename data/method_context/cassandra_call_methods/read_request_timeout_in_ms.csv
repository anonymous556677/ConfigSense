function,option,Method,Method_short,class_name,xml_path,Method_body
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(O)org.apache.cassandra.utils.concurrent.OpOrder:<init>(),<init>,OpOrder,../data/xml/cassandra_call_methods/OpOrder.xml,not found
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(O)org.apache.cassandra.db.DiskBoundaryManager:<init>(),<init>,DiskBoundaryManager,../data/xml/cassandra_call_methods/DiskBoundaryManager.xml,not found
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(M)org.apache.cassandra.schema.TableMetadataRef:get(),get,TableMetadataRef,../data/xml/cassandra_call_methods/TableMetadataRef.xml,"
public TableMetadata get()
    {
        return metadata;
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(M)org.apache.cassandra.schema.CompactionParams:minCompactionThreshold(),minCompactionThreshold,CompactionParams,../data/xml/cassandra_call_methods/CompactionParams.xml,"
public int minCompactionThreshold()
    {
        String threshold = options.get(Option.MIN_THRESHOLD.toString());
        return threshold == null
             ? DEFAULT_MIN_THRESHOLD
             : Integer.parseInt(threshold);
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(M)org.apache.cassandra.schema.CompactionParams:maxCompactionThreshold(),maxCompactionThreshold,CompactionParams,../data/xml/cassandra_call_methods/CompactionParams.xml,"
public int maxCompactionThreshold()
    {
        String threshold = options.get(Option.MAX_THRESHOLD.toString());
        return threshold == null
             ? DEFAULT_MAX_THRESHOLD
             : Integer.parseInt(threshold);
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(M)org.apache.cassandra.db.view.ViewManager:forTable(org.apache.cassandra.schema.TableId),forTable,ViewManager,../data/xml/cassandra_call_methods/ViewManager.xml,"
public TableViews forTable(TableId id)
    {
        TableViews views = viewsByBaseTable.get(id);
        if (views == null)
        {
            views = new TableViews(id);
            TableViews previous = viewsByBaseTable.putIfAbsent(id, views);
            if (previous != null)
                views = previous;
        }
        return views;
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(M)org.apache.cassandra.db.Keyspace:getName(),getName,Keyspace,../data/xml/cassandra_call_methods/Keyspace.xml,"
public String getName()
    {
        return metadata.name;
    }
}"
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(S)org.apache.cassandra.config.DatabaseDescriptor:isDaemonInitialized(),isDaemonInitialized,DatabaseDescriptor,../data/xml/cassandra_call_methods/DatabaseDescriptor.xml,"
public static boolean isDaemonInitialized()
    {
        return daemonInitialized;
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(M)org.apache.cassandra.db.commitlog.CommitLog:getCurrentPosition(),getCurrentPosition,CommitLog,../data/xml/cassandra_call_methods/CommitLog.xml,"/**
     * @return a CommitLogPosition which, if {@code >= one} returned from add(), implies add() was started
     * (but not necessarily finished) prior to this call
     */
public CommitLogPosition getCurrentPosition()
    {
        return segmentManager.getCurrentPosition();
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,"(O)org.apache.cassandra.db.lifecycle.Tracker:<init>(org.apache.cassandra.db.Memtable,boolean)",<init>,Tracker,../data/xml/cassandra_call_methods/Tracker.xml,"/**
     * @param memtable Initial Memtable. Can be null.
     * @param loadsstables true to indicate to load SSTables (TODO: remove as this is only accessed from 2i)
     */
public Tracker(Memtable memtable, boolean loadsstables)
    {
        this.cfstore = memtable != null ? memtable.cfs : null;
        this.view = new AtomicReference<>();
        this.loadsstables = loadsstables;
        this.reset(memtable);
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(M)org.apache.cassandra.db.lifecycle.Tracker:subscribe(org.apache.cassandra.notifications.INotificationConsumer),subscribe,Tracker,../data/xml/cassandra_call_methods/Tracker.xml,"
public void subscribe(INotificationConsumer consumer)
    {
        subscribers.add(consumer);
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(M)org.apache.cassandra.db.Directories:sstableLister(org.apache.cassandra.db.Directories$OnTxnErr),sstableLister,Directories,../data/xml/cassandra_call_methods/Directories.xml,"
public SSTableLister sstableLister(OnTxnErr onTxnErr)
    {
        return new SSTableLister(this.dataPaths, this.metadata, onTxnErr);
    }

    

public SSTableLister sstableLister(File directory, OnTxnErr onTxnErr)
    {
        return new SSTableLister(new File[]{directory}, metadata, onTxnErr);
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(M)org.apache.cassandra.db.Directories$SSTableLister:skipTemporary(boolean),skipTemporary,Directories$SSTableLister,../data/xml/cassandra_call_methods/Directories.xml,"
public SSTableLister skipTemporary(boolean b)
        {
            if (filtered)
                throw new IllegalStateException(""list() has already been called"");
            skipTemporary = b;
            return this;
        }

        "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(M)org.apache.cassandra.db.Directories$SSTableLister:list(),list,Directories$SSTableLister,../data/xml/cassandra_call_methods/Directories.xml,"
public Map<Descriptor, Set<Component>> list()
        {
            filter();
            return ImmutableMap.copyOf(components);
        }

        "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(O)org.apache.cassandra.db.compaction.CompactionStrategyManager:<init>(org.apache.cassandra.db.ColumnFamilyStore),<init>,CompactionStrategyManager,../data/xml/cassandra_call_methods/CompactionStrategyManager.xml,"
public CompactionStrategyManager(ColumnFamilyStore cfs)
    {
        this(cfs, cfs::getDiskBoundaries, cfs.getPartitioner().splitter().isPresent());
    }

    

@VisibleForTesting
    public CompactionStrategyManager(ColumnFamilyStore cfs, Supplier<DiskBoundaries> boundariesSupplier,
                                     boolean partitionSSTablesByTokenRange)
    {
        AbstractStrategyHolder.DestinationRouter router = new AbstractStrategyHolder.DestinationRouter()
        {
            public int getIndexForSSTable(SSTableReader sstable)
            {
                return compactionStrategyIndexFor(sstable);
            }

            public int getIndexForSSTableDirectory(Descriptor descriptor)
            {
                return compactionStrategyIndexForDirectory(descriptor);
            }
        };
        transientRepairs = new PendingRepairHolder(cfs, router, true);
        pendingRepairs = new PendingRepairHolder(cfs, router, false);
        repaired = new CompactionStrategyHolder(cfs, router, true);
        unrepaired = new CompactionStrategyHolder(cfs, router, false);
        holders = ImmutableList.of(transientRepairs, pendingRepairs, repaired, unrepaired);

        cfs.getTracker().subscribe(this);
        logger.trace(""{} subscribed to the data tracker."", this);
        this.cfs = cfs;
        this.compactionLogger = new CompactionLogger(cfs, this);
        this.boundariesSupplier = boundariesSupplier;
        this.partitionSSTablesByTokenRange = partitionSSTablesByTokenRange;
        params = cfs.metadata().params.compaction;
        enabled = params.isEnabled();
        reload(cfs.metadata().params.compaction);
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(M)org.apache.cassandra.utils.DefaultValue:value(),value,DefaultValue,../data/xml/cassandra_call_methods/DefaultValue.xml,"
public T value()
    {
        return currentValue;
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(M)org.apache.cassandra.db.compaction.CompactionStrategyManager:disable(),disable,CompactionStrategyManager,../data/xml/cassandra_call_methods/CompactionStrategyManager.xml,"
public void disable()
    {
        writeLock.lock();
        try
        {
            enabled = false;
        }
        finally
        {
            writeLock.unlock();
        }
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(O)org.apache.cassandra.index.SecondaryIndexManager:<init>(org.apache.cassandra.db.ColumnFamilyStore),<init>,SecondaryIndexManager,../data/xml/cassandra_call_methods/SecondaryIndexManager.xml,"
public SecondaryIndexManager(ColumnFamilyStore baseCfs)
    {
        this.baseCfs = baseCfs;
        this.keyspace = baseCfs.keyspace;
        baseCfs.getTracker().subscribe(this);
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(M)org.apache.cassandra.schema.Indexes:iterator(),iterator,Indexes,../data/xml/cassandra_call_methods/Indexes.xml,"
public Iterator<IndexMetadata> iterator()
    {
        return indexesByName.values().iterator();
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,"(M)org.apache.cassandra.index.SecondaryIndexManager:addIndex(org.apache.cassandra.schema.IndexMetadata,boolean)",addIndex,SecondaryIndexManager,../data/xml/cassandra_call_methods/SecondaryIndexManager.xml,"/**
     * Adds and builds a index
     *
     * @param indexDef the IndexMetadata describing the index
     * @param isNewCF true if the index is added as part of a new table/columnfamily (i.e. loading a CF at startup), 
     * false for all other cases (i.e. newly added index)
     */
public synchronized Future<?> addIndex(IndexMetadata indexDef, boolean isNewCF)
    {
        if (indexes.containsKey(indexDef.name))
            return reloadIndex(indexDef);
        else
            return createIndex(indexDef, isNewCF);
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(O)org.apache.cassandra.metrics.TableMetrics:<init>(org.apache.cassandra.db.ColumnFamilyStore),<init>,TableMetrics,../data/xml/cassandra_call_methods/TableMetrics.xml,"/**
     * Creates metrics for given {@link ColumnFamilyStore}.
     *
     * @param cfs ColumnFamilyStore to measure metrics
     */
public TableMetrics(final ColumnFamilyStore cfs)
    {
        factory = new TableMetricNameFactory(cfs, ""Table"");
        aliasFactory = new TableMetricNameFactory(cfs, ""ColumnFamily"");

        samplers = new EnumMap<>(SamplerType.class);
        topReadPartitionFrequency = new FrequencySampler<ByteBuffer>()
        {
            public String toString(ByteBuffer value)
            {
                return cfs.metadata().partitionKeyType.getString(value);
            }
        };
        topWritePartitionFrequency = new FrequencySampler<ByteBuffer>()
        {
            public String toString(ByteBuffer value)
            {
                return cfs.metadata().partitionKeyType.getString(value);
            }
        };
        topWritePartitionSize = new MaxSampler<ByteBuffer>()
        {
            public String toString(ByteBuffer value)
            {
                return cfs.metadata().partitionKeyType.getString(value);
            }
        };
        topCasPartitionContention = new FrequencySampler<ByteBuffer>()
        {
            public String toString(ByteBuffer value)
            {
                return cfs.metadata().partitionKeyType.getString(value);
            }
        };
        topLocalReadQueryTime = new MaxSampler<String>()
        {
            public String toString(String value)
            {
                return value;
            }
        };

        samplers.put(SamplerType.READS, topReadPartitionFrequency);
        samplers.put(SamplerType.WRITES, topWritePartitionFrequency);
        samplers.put(SamplerType.WRITE_SIZE, topWritePartitionSize);
        samplers.put(SamplerType.CAS_CONTENTIONS, topCasPartitionContention);
        samplers.put(SamplerType.LOCAL_READ_TIME, topLocalReadQueryTime);

        memtableColumnsCount = createTableGauge(""MemtableColumnsCount"", 
                                                () -> cfs.getTracker().getView().getCurrentMemtable().getOperations());

        // MemtableOnHeapSize naming deprecated in 4.0
        memtableOnHeapDataSize = createTableGaugeWithDeprecation(""MemtableOnHeapDataSize"", ""MemtableOnHeapSize"", 
                                                                 () -> cfs.getTracker().getView().getCurrentMemtable().getAllocator().onHeap().owns(), 
                                                                 new GlobalTableGauge(""MemtableOnHeapDataSize""));

        // MemtableOffHeapSize naming deprecated in 4.0
        memtableOffHeapDataSize = createTableGaugeWithDeprecation(""MemtableOffHeapDataSize"", ""MemtableOffHeapSize"", 
                                                                  () -> cfs.getTracker().getView().getCurrentMemtable().getAllocator().offHeap().owns(), 
                                                                  new GlobalTableGauge(""MemtableOnHeapDataSize""));
        
        memtableLiveDataSize = createTableGauge(""MemtableLiveDataSize"", 
                                                () -> cfs.getTracker().getView().getCurrentMemtable().getLiveDataSize());

        // AllMemtablesHeapSize naming deprecated in 4.0
        allMemtablesOnHeapDataSize = createTableGaugeWithDeprecation(""AllMemtablesOnHeapDataSize"", ""AllMemtablesHeapSize"", new Gauge<Long>()
        {
            public Long getValue()
            {
                long size = 0;
                for (ColumnFamilyStore cfs2 : cfs.concatWithIndexes())
                    size += cfs2.getTracker().getView().getCurrentMemtable().getAllocator().onHeap().owns();
                return size;
            }
        }, new GlobalTableGauge(""AllMemtablesOnHeapDataSize""));

        // AllMemtablesOffHeapSize naming deprecated in 4.0
        allMemtablesOffHeapDataSize = createTableGaugeWithDeprecation(""AllMemtablesOffHeapDataSize"", ""AllMemtablesOffHeapSize"", new Gauge<Long>()
        {
            public Long getValue()
            {
                long size = 0;
                for (ColumnFamilyStore cfs2 : cfs.concatWithIndexes())
                    size += cfs2.getTracker().getView().getCurrentMemtable().getAllocator().offHeap().owns();
                return size;
            }
        }, new GlobalTableGauge(""AllMemtablesOffHeapDataSize""));
        allMemtablesLiveDataSize = createTableGauge(""AllMemtablesLiveDataSize"", new Gauge<Long>()
        {
            public Long getValue()
            {
                long size = 0;
                for (ColumnFamilyStore cfs2 : cfs.concatWithIndexes())
                    size += cfs2.getTracker().getView().getCurrentMemtable().getLiveDataSize();
                return size;
            }
        });
        memtableSwitchCount = createTableCounter(""MemtableSwitchCount"");
        estimatedPartitionSizeHistogram = createTableGauge(""EstimatedPartitionSizeHistogram"", ""EstimatedRowSizeHistogram"",
                                                           () -> combineHistograms(cfs.getSSTables(SSTableSet.CANONICAL),
                                                                                   SSTableReader::getEstimatedPartitionSize), null);
        
        estimatedPartitionCount = createTableGauge(""EstimatedPartitionCount"", ""EstimatedRowCount"", new Gauge<Long>()
        {
            public Long getValue()
            {
                long memtablePartitions = 0;
                for (Memtable memtable : cfs.getTracker().getView().getAllMemtables())
                   memtablePartitions += memtable.partitionCount();
                try(ColumnFamilyStore.RefViewFragment refViewFragment = cfs.selectAndReference(View.selectFunction(SSTableSet.CANONICAL)))
                {
                    return SSTableReader.getApproximateKeyCount(refViewFragment.sstables) + memtablePartitions;
                }
            }
        }, null);
        estimatedColumnCountHistogram = createTableGauge(""EstimatedColumnCountHistogram"", ""EstimatedColumnCountHistogram"",
                                                         () -> combineHistograms(cfs.getSSTables(SSTableSet.CANONICAL), 
                                                                                 SSTableReader::getEstimatedCellPerPartitionCount), null);
        
        sstablesPerReadHistogram = createTableHistogram(""SSTablesPerReadHistogram"", cfs.keyspace.metric.sstablesPerReadHistogram, true);
        compressionRatio = createTableGauge(""CompressionRatio"", new Gauge<Double>()
        {
            public Double getValue()
            {
                return computeCompressionRatio(cfs.getSSTables(SSTableSet.CANONICAL));
            }
        }, new Gauge<Double>() // global gauge
        {
            public Double getValue()
            {
                List<SSTableReader> sstables = new ArrayList<>();
                Keyspace.all().forEach(ks -> sstables.addAll(ks.getAllSSTables(SSTableSet.CANONICAL)));
                return computeCompressionRatio(sstables);
            }
        });
        percentRepaired = createTableGauge(""PercentRepaired"", new Gauge<Double>()
        {
            public Double getValue()
            {
                double repaired = 0;
                double total = 0;
                for (SSTableReader sstable : cfs.getSSTables(SSTableSet.CANONICAL))
                {
                    if (sstable.isRepaired())
                    {
                        repaired += sstable.uncompressedLength();
                    }
                    total += sstable.uncompressedLength();
                }
                return total > 0 ? (repaired / total) * 100 : 100.0;
            }
        });

        bytesRepaired = createTableGauge(""BytesRepaired"", new Gauge<Long>()
        {
            public Long getValue()
            {
                long size = 0;
                for (SSTableReader sstable: Iterables.filter(cfs.getSSTables(SSTableSet.CANONICAL), SSTableReader::isRepaired))
                {
                    size += sstable.uncompressedLength();
                }
                return size;
            }
        });

        bytesUnrepaired = createTableGauge(""BytesUnrepaired"", new Gauge<Long>()
        {
            public Long getValue()
            {
                long size = 0;
                for (SSTableReader sstable: Iterables.filter(cfs.getSSTables(SSTableSet.CANONICAL), s -> !s.isRepaired() && !s.isPendingRepair()))
                {
                    size += sstable.uncompressedLength();
                }
                return size;
            }
        });

        bytesPendingRepair = createTableGauge(""BytesPendingRepair"", new Gauge<Long>()
        {
            public Long getValue()
            {
                long size = 0;
                for (SSTableReader sstable: Iterables.filter(cfs.getSSTables(SSTableSet.CANONICAL), SSTableReader::isPendingRepair))
                {
                    size += sstable.uncompressedLength();
                }
                return size;
            }
        });

        readLatency = createLatencyMetrics(""Read"", cfs.keyspace.metric.readLatency, GLOBAL_READ_LATENCY);
        writeLatency = createLatencyMetrics(""Write"", cfs.keyspace.metric.writeLatency, GLOBAL_WRITE_LATENCY);
        rangeLatency = createLatencyMetrics(""Range"", cfs.keyspace.metric.rangeLatency, GLOBAL_RANGE_LATENCY);
        pendingFlushes = createTableCounter(""PendingFlushes"");
        bytesFlushed = createTableCounter(""BytesFlushed"");

        compactionBytesWritten = createTableCounter(""CompactionBytesWritten"");
        pendingCompactions = createTableGauge(""PendingCompactions"", () -> cfs.getCompactionStrategyManager().getEstimatedRemainingTasks());
        liveSSTableCount = createTableGauge(""LiveSSTableCount"", () -> cfs.getTracker().getView().liveSSTables().size());
        oldVersionSSTableCount = createTableGauge(""OldVersionSSTableCount"", new Gauge<Integer>()
        {
            public Integer getValue()
            {
                int count = 0;
                for (SSTableReader sstable : cfs.getLiveSSTables())
                    if (!sstable.descriptor.version.isLatestVersion())
                        count++;
                return count;
            }
        });
        liveDiskSpaceUsed = createTableCounter(""LiveDiskSpaceUsed"");
        totalDiskSpaceUsed = createTableCounter(""TotalDiskSpaceUsed"");
        minPartitionSize = createTableGauge(""MinPartitionSize"", ""MinRowSize"", new Gauge<Long>()
        {
            public Long getValue()
            {
                long min = 0;
                for (SSTableReader sstable : cfs.getSSTables(SSTableSet.CANONICAL))
                {
                    if (min == 0 || sstable.getEstimatedPartitionSize().min() < min)
                        min = sstable.getEstimatedPartitionSize().min();
                }
                return min;
            }
        }, new Gauge<Long>() // global gauge
        {
            public Long getValue()
            {
                long min = Long.MAX_VALUE;
                for (Metric cfGauge : ALL_TABLE_METRICS.get(""MinPartitionSize""))
                {
                    min = Math.min(min, ((Gauge<? extends Number>) cfGauge).getValue().longValue());
                }
                return min;
            }
        });
        maxPartitionSize = createTableGauge(""MaxPartitionSize"", ""MaxRowSize"", new Gauge<Long>()
        {
            public Long getValue()
            {
                long max = 0;
                for (SSTableReader sstable : cfs.getSSTables(SSTableSet.CANONICAL))
                {
                    if (sstable.getEstimatedPartitionSize().max() > max)
                        max = sstable.getEstimatedPartitionSize().max();
                }
                return max;
            }
        }, new Gauge<Long>() // global gauge
        {
            public Long getValue()
            {
                long max = 0;
                for (Metric cfGauge : ALL_TABLE_METRICS.get(""MaxPartitionSize""))
                {
                    max = Math.max(max, ((Gauge<? extends Number>) cfGauge).getValue().longValue());
                }
                return max;
            }
        });
        meanPartitionSize = createTableGauge(""MeanPartitionSize"", ""MeanRowSize"", new Gauge<Long>()
        {
            public Long getValue()
            {
                long sum = 0;
                long count = 0;
                for (SSTableReader sstable : cfs.getSSTables(SSTableSet.CANONICAL))
                {
                    long n = sstable.getEstimatedPartitionSize().count();
                    sum += sstable.getEstimatedPartitionSize().mean() * n;
                    count += n;
                }
                return count > 0 ? sum / count : 0;
            }
        }, new Gauge<Long>() // global gauge
        {
            public Long getValue()
            {
                long sum = 0;
                long count = 0;
                for (Keyspace keyspace : Keyspace.all())
                {
                    for (SSTableReader sstable : keyspace.getAllSSTables(SSTableSet.CANONICAL))
                    {
                        long n = sstable.getEstimatedPartitionSize().count();
                        sum += sstable.getEstimatedPartitionSize().mean() * n;
                        count += n;
                    }
                }
                return count > 0 ? sum / count : 0;
            }
        });
        bloomFilterFalsePositives = createTableGauge(""BloomFilterFalsePositives"", new Gauge<Long>()
        {
            public Long getValue()
            {
                long count = 0L;
                for (SSTableReader sstable: cfs.getSSTables(SSTableSet.LIVE))
                    count += sstable.getBloomFilterFalsePositiveCount();
                return count;
            }
        });
        recentBloomFilterFalsePositives = createTableGauge(""RecentBloomFilterFalsePositives"", new Gauge<Long>()
        {
            public Long getValue()
            {
                long count = 0L;
                for (SSTableReader sstable : cfs.getSSTables(SSTableSet.LIVE))
                    count += sstable.getRecentBloomFilterFalsePositiveCount();
                return count;
            }
        });
        bloomFilterFalseRatio = createTableGauge(""BloomFilterFalseRatio"", new Gauge<Double>()
        {
            public Double getValue()
            {
                long falsePositiveCount = 0L;
                long truePositiveCount = 0L;
                long trueNegativeCount = 0L;
                for (SSTableReader sstable : cfs.getSSTables(SSTableSet.LIVE))
                {
                    falsePositiveCount += sstable.getBloomFilterFalsePositiveCount();
                    truePositiveCount += sstable.getBloomFilterTruePositiveCount();
                    trueNegativeCount += sstable.getBloomFilterTrueNegativeCount();
                }
                if (falsePositiveCount == 0L && truePositiveCount == 0L)
                    return 0d;
                return (double) falsePositiveCount / (truePositiveCount + falsePositiveCount + trueNegativeCount);
            }
        }, new Gauge<Double>() // global gauge
        {
            public Double getValue()
            {
                long falsePositiveCount = 0L;
                long truePositiveCount = 0L;
                long trueNegativeCount = 0L;
                for (Keyspace keyspace : Keyspace.all())
                {
                    for (SSTableReader sstable : keyspace.getAllSSTables(SSTableSet.LIVE))
                    {
                        falsePositiveCount += sstable.getBloomFilterFalsePositiveCount();
                        truePositiveCount += sstable.getBloomFilterTruePositiveCount();
                        trueNegativeCount += sstable.getBloomFilterTrueNegativeCount();
                    }
                }
                if (falsePositiveCount == 0L && truePositiveCount == 0L)
                    return 0d;
                return (double) falsePositiveCount / (truePositiveCount + falsePositiveCount + trueNegativeCount);
            }
        });
        recentBloomFilterFalseRatio = createTableGauge(""RecentBloomFilterFalseRatio"", new Gauge<Double>()
        {
            public Double getValue()
            {
                long falsePositiveCount = 0L;
                long truePositiveCount = 0L;
                long trueNegativeCount = 0L;
                for (SSTableReader sstable: cfs.getSSTables(SSTableSet.LIVE))
                {
                    falsePositiveCount += sstable.getRecentBloomFilterFalsePositiveCount();
                    truePositiveCount += sstable.getRecentBloomFilterTruePositiveCount();
                    trueNegativeCount += sstable.getRecentBloomFilterTrueNegativeCount();
                }
                if (falsePositiveCount == 0L && truePositiveCount == 0L)
                    return 0d;
                return (double) falsePositiveCount / (truePositiveCount + falsePositiveCount + trueNegativeCount);
            }
        }, new Gauge<Double>() // global gauge
        {
            public Double getValue()
            {
                long falsePositiveCount = 0L;
                long truePositiveCount = 0L;
                long trueNegativeCount = 0L;
                for (Keyspace keyspace : Keyspace.all())
                {
                    for (SSTableReader sstable : keyspace.getAllSSTables(SSTableSet.LIVE))
                    {
                        falsePositiveCount += sstable.getRecentBloomFilterFalsePositiveCount();
                        truePositiveCount += sstable.getRecentBloomFilterTruePositiveCount();
                        trueNegativeCount += sstable.getRecentBloomFilterTrueNegativeCount();
                    }
                }
                if (falsePositiveCount == 0L && truePositiveCount == 0L)
                    return 0d;
                return (double) falsePositiveCount / (truePositiveCount + falsePositiveCount + trueNegativeCount);
            }
        });
        bloomFilterDiskSpaceUsed = createTableGauge(""BloomFilterDiskSpaceUsed"", new Gauge<Long>()
        {
            public Long getValue()
            {
                long total = 0;
                for (SSTableReader sst : cfs.getSSTables(SSTableSet.CANONICAL))
                    total += sst.getBloomFilterSerializedSize();
                return total;
            }
        });
        bloomFilterOffHeapMemoryUsed = createTableGauge(""BloomFilterOffHeapMemoryUsed"", new Gauge<Long>()
        {
            public Long getValue()
            {
                long total = 0;
                for (SSTableReader sst : cfs.getSSTables(SSTableSet.LIVE))
                    total += sst.getBloomFilterOffHeapSize();
                return total;
            }
        });
        indexSummaryOffHeapMemoryUsed = createTableGauge(""IndexSummaryOffHeapMemoryUsed"", new Gauge<Long>()
        {
            public Long getValue()
            {
                long total = 0;
                for (SSTableReader sst : cfs.getSSTables(SSTableSet.LIVE))
                    total += sst.getIndexSummaryOffHeapSize();
                return total;
            }
        });
        compressionMetadataOffHeapMemoryUsed = createTableGauge(""CompressionMetadataOffHeapMemoryUsed"", new Gauge<Long>()
        {
            public Long getValue()
            {
                long total = 0;
                for (SSTableReader sst : cfs.getSSTables(SSTableSet.LIVE))
                    total += sst.getCompressionMetadataOffHeapSize();
                return total;
            }
        });
        speculativeRetries = createTableCounter(""SpeculativeRetries"");
        speculativeFailedRetries = createTableCounter(""SpeculativeFailedRetries"");
        speculativeInsufficientReplicas = createTableCounter(""SpeculativeInsufficientReplicas"");
        speculativeSampleLatencyNanos = createTableGauge(""SpeculativeSampleLatencyNanos"", () -> cfs.sampleReadLatencyNanos);

        additionalWrites = createTableCounter(""AdditionalWrites"");
        additionalWriteLatencyNanos = createTableGauge(""AdditionalWriteLatencyNanos"", () -> cfs.additionalWriteLatencyNanos);

        keyCacheHitRate = createTableGauge(""KeyCacheHitRate"", ""KeyCacheHitRate"", new RatioGauge()
        {
            @Override
            public Ratio getRatio()
            {
                return Ratio.of(getNumerator(), getDenominator());
            }

            protected double getNumerator()
            {
                long hits = 0L;
                for (SSTableReader sstable : cfs.getSSTables(SSTableSet.LIVE))
                    hits += sstable.getKeyCacheHit();
                return hits;
            }

            protected double getDenominator()
            {
                long requests = 0L;
                for (SSTableReader sstable : cfs.getSSTables(SSTableSet.LIVE))
                    requests += sstable.getKeyCacheRequest();
                return Math.max(requests, 1); // to avoid NaN.
            }
        }, null);
        tombstoneScannedHistogram = createTableHistogram(""TombstoneScannedHistogram"", cfs.keyspace.metric.tombstoneScannedHistogram, false);
        liveScannedHistogram = createTableHistogram(""LiveScannedHistogram"", cfs.keyspace.metric.liveScannedHistogram, false);
        colUpdateTimeDeltaHistogram = createTableHistogram(""ColUpdateTimeDeltaHistogram"", cfs.keyspace.metric.colUpdateTimeDeltaHistogram, false);
        coordinatorReadLatency = createTableTimer(""CoordinatorReadLatency"");
        coordinatorScanLatency = createTableTimer(""CoordinatorScanLatency"");
        coordinatorWriteLatency = createTableTimer(""CoordinatorWriteLatency"");
        waitingOnFreeMemtableSpace = createTableHistogram(""WaitingOnFreeMemtableSpace"", false);

        // We do not want to capture view mutation specific metrics for a view
        // They only makes sense to capture on the base table
        if (cfs.metadata().isView())
        {
            viewLockAcquireTime = null;
            viewReadTime = null;
        }
        else
        {
            viewLockAcquireTime = createTableTimer(""ViewLockAcquireTime"", cfs.keyspace.metric.viewLockAcquireTime);
            viewReadTime = createTableTimer(""ViewReadTime"", cfs.keyspace.metric.viewReadTime);
        }

        trueSnapshotsSize = createTableGauge(""SnapshotsSize"", cfs::trueSnapshotsSize);
        rowCacheHitOutOfRange = createTableCounter(""RowCacheHitOutOfRange"");
        rowCacheHit = createTableCounter(""RowCacheHit"");
        rowCacheMiss = createTableCounter(""RowCacheMiss"");

        tombstoneFailures = createTableCounter(""TombstoneFailures"");
        tombstoneWarnings = createTableCounter(""TombstoneWarnings"");

        droppedMutations = createTableCounter(""DroppedMutations"");

        casPrepare = createLatencyMetrics(""CasPrepare"", cfs.keyspace.metric.casPrepare);
        casPropose = createLatencyMetrics(""CasPropose"", cfs.keyspace.metric.casPropose);
        casCommit = createLatencyMetrics(""CasCommit"", cfs.keyspace.metric.casCommit);

        repairsStarted = createTableCounter(""RepairJobsStarted"");
        repairsCompleted = createTableCounter(""RepairJobsCompleted"");

        anticompactionTime = createTableTimer(""AnticompactionTime"", cfs.keyspace.metric.anticompactionTime);
        validationTime = createTableTimer(""ValidationTime"", cfs.keyspace.metric.validationTime);
        repairSyncTime = createTableTimer(""RepairSyncTime"", cfs.keyspace.metric.repairSyncTime);

        bytesValidated = createTableHistogram(""BytesValidated"", cfs.keyspace.metric.bytesValidated, false);
        partitionsValidated = createTableHistogram(""PartitionsValidated"", cfs.keyspace.metric.partitionsValidated, false);
        bytesAnticompacted = createTableCounter(""BytesAnticompacted"");
        bytesMutatedAnticompaction = createTableCounter(""BytesMutatedAnticompaction"");
        mutatedAnticompactionGauge = createTableGauge(""MutatedAnticompactionGauge"", () ->
        {
            double bytesMutated = bytesMutatedAnticompaction.getCount();
            double bytesAnticomp = bytesAnticompacted.getCount();
            if (bytesAnticomp + bytesMutated > 0)
                return bytesMutated / (bytesAnticomp + bytesMutated);
            return 0.0;
        });

        readRepairRequests = createTableMeter(""ReadRepairRequests"");
        shortReadProtectionRequests = createTableMeter(""ShortReadProtectionRequests"");
        replicaFilteringProtectionRequests = createTableMeter(""ReplicaFilteringProtectionRequests"");
        rfpRowsCachedPerQuery = createHistogram(""ReplicaFilteringProtectionRowsCachedPerQuery"", true);

        confirmedRepairedInconsistencies = createTableMeter(""RepairedDataInconsistenciesConfirmed"", cfs.keyspace.metric.confirmedRepairedInconsistencies);
        unconfirmedRepairedInconsistencies = createTableMeter(""RepairedDataInconsistenciesUnconfirmed"", cfs.keyspace.metric.unconfirmedRepairedInconsistencies);

        repairedDataTrackingOverreadRows = createTableHistogram(""RepairedDataTrackingOverreadRows"", cfs.keyspace.metric.repairedDataTrackingOverreadRows, false);
        repairedDataTrackingOverreadTime = createTableTimer(""RepairedDataTrackingOverreadTime"", cfs.keyspace.metric.repairedDataTrackingOverreadTime);

        unleveledSSTables = createTableGauge(""UnleveledSSTables"", cfs::getUnleveledSSTables, () -> {
            // global gauge
            int cnt = 0;
            for (Metric cfGauge : ALL_TABLE_METRICS.get(""UnleveledSSTables""))
            {
                cnt += ((Gauge<? extends Number>) cfGauge).getValue().intValue();
            }
            return cnt;
        });
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(M)org.apache.cassandra.db.ColumnFamilyStore:isIndex(),isIndex,ColumnFamilyStore,../data/xml/cassandra_call_methods/ColumnFamilyStore.xml,"/** true if this CFS contains secondary index data */
public boolean isIndex()
    {
        return metadata().isIndex();
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(O)org.apache.cassandra.db.CassandraTableWriteHandler:<init>(org.apache.cassandra.db.ColumnFamilyStore),<init>,CassandraTableWriteHandler,../data/xml/cassandra_call_methods/CassandraTableWriteHandler.xml,"
public CassandraTableWriteHandler(ColumnFamilyStore cfs)
    {
        this.cfs = cfs;
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(O)org.apache.cassandra.db.streaming.CassandraStreamManager:<init>(org.apache.cassandra.db.ColumnFamilyStore),<init>,CassandraStreamManager,../data/xml/cassandra_call_methods/CassandraStreamManager.xml,"
public CassandraStreamManager(ColumnFamilyStore cfs)
    {
        this.cfs = cfs;
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(O)org.apache.cassandra.db.repair.CassandraTableRepairManager:<init>(org.apache.cassandra.db.ColumnFamilyStore),<init>,CassandraTableRepairManager,../data/xml/cassandra_call_methods/CassandraTableRepairManager.xml,"
public CassandraTableRepairManager(ColumnFamilyStore cfs)
    {
        this.cfs = cfs;
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(O)org.apache.cassandra.db.SSTableImporter:<init>(org.apache.cassandra.db.ColumnFamilyStore),<init>,SSTableImporter,../data/xml/cassandra_call_methods/SSTableImporter.xml,"
public SSTableImporter(ColumnFamilyStore cfs)
    {
        this.cfs = cfs;
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(O)org.apache.cassandra.service.DefaultFSErrorHandler:<init>(),<init>,DefaultFSErrorHandler,../data/xml/cassandra_call_methods/DefaultFSErrorHandler.xml,not found
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(S)org.apache.cassandra.io.util.FileUtils:setFSErrorHandler(org.apache.cassandra.io.FSErrorHandler),setFSErrorHandler,FileUtils,../data/xml/cassandra_call_methods/FileUtils.xml,"
public static void setFSErrorHandler(FSErrorHandler handler)
    {
        fsErrorHandler.getAndSet(Optional.ofNullable(handler));
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(M)org.apache.cassandra.service.CassandraDaemon:migrateSystemDataIfNeeded(),migrateSystemDataIfNeeded,CassandraDaemon,../data/xml/cassandra_call_methods/CassandraDaemon.xml,"/**
     * Checks if the data of the local system keyspaces need to be migrated to a different location.
     *
     * @throws IOException
     */
public void migrateSystemDataIfNeeded() throws IOException
    {
        // If there is only one directory and no system keyspace directory has been specified we do not need to do
        // anything. If it is not the case we want to try to migrate the data.
        if (!DatabaseDescriptor.useSpecificLocationForLocalSystemData()
                && DatabaseDescriptor.getNonLocalSystemKeyspacesDataFileLocations().length <= 1)
            return;

        // We can face several cases:
        //  1) The system data are spread accross the data file locations and need to be moved to
        //     the first data location (upgrade to 4.0)
        //  2) The system data are spread accross the data file locations and need to be moved to
        //     the system keyspace location configured by the user (upgrade to 4.0)
        //  3) The system data are stored in the first data location and need to be moved to
        //     the system keyspace location configured by the user (system_data_file_directory has been configured)
        Path target = Paths.get(DatabaseDescriptor.getLocalSystemKeyspacesDataFileLocations()[0]);

        String[] nonLocalSystemKeyspacesFileLocations = DatabaseDescriptor.getNonLocalSystemKeyspacesDataFileLocations();
        String[] sources = DatabaseDescriptor.useSpecificLocationForLocalSystemData() ? nonLocalSystemKeyspacesFileLocations
                                                                                      : Arrays.copyOfRange(nonLocalSystemKeyspacesFileLocations,
                                                                                                           1,
                                                                                                           nonLocalSystemKeyspacesFileLocations.length);

        for (String source : sources)
        {
            Path dataFileLocation = Paths.get(source);

            if (!Files.exists(dataFileLocation))
                continue;

            try (Stream<Path> locationChildren = Files.list(dataFileLocation))
            {
                Path[] keyspaceDirectories = locationChildren.filter(p -> SchemaConstants.isLocalSystemKeyspace(p.getFileName().toString()))
                                                             .toArray(Path[]::new);

                for (Path keyspaceDirectory : keyspaceDirectories)
                {
                    try (Stream<Path> keyspaceChildren = Files.list(keyspaceDirectory))
                    {
                        Path[] tableDirectories = keyspaceChildren.filter(Files::isDirectory)
                                                                  .filter(p -> !SystemKeyspace.TABLES_SPLIT_ACROSS_MULTIPLE_DISKS
                                                                                              .contains(p.getFileName()
                                                                                                         .toString()))
                                                                  .toArray(Path[]::new);

                        for (Path tableDirectory : tableDirectories)
                        {
                            FileUtils.moveRecursively(tableDirectory,
                                                      target.resolve(dataFileLocation.relativize(tableDirectory)));
                        }

                        if (!SchemaConstants.SYSTEM_KEYSPACE_NAME.equals(keyspaceDirectory.getFileName().toString()))
                        {
                            FileUtils.deleteDirectoryIfEmpty(keyspaceDirectory);
                        }
                    }
                }
            }
        }
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(S)org.apache.cassandra.db.WindowsFailedSnapshotTracker:deleteOldSnapshots(),deleteOldSnapshots,WindowsFailedSnapshotTracker,../data/xml/cassandra_call_methods/WindowsFailedSnapshotTracker.xml,"
public static void deleteOldSnapshots()
    {
        if (new File(TODELETEFILE).exists())
        {
            try
            {
                try (BufferedReader reader = new BufferedReader(new FileReader(TODELETEFILE)))
                {
                    String snapshotDirectory;
                    while ((snapshotDirectory = reader.readLine()) != null)
                    {
                        File f = new File(snapshotDirectory);

                        // Skip folders that aren't a subset of temp or a data folder. We don't want people to accidentally
                        // delete something important by virtue of adding something invalid to the .toDelete file.
                        boolean validFolder = FileUtils.isSubDirectory(new File(System.getenv(""TEMP"")), f);
                        for (String s : DatabaseDescriptor.getAllDataFileLocations())
                            validFolder |= FileUtils.isSubDirectory(new File(s), f);

                        if (!validFolder)
                        {
                            logger.warn(""Skipping invalid directory found in .toDelete: {}. Only %TEMP% or data file subdirectories are valid."", f);
                            continue;
                        }

                        // Could be a non-existent directory if deletion worked on previous JVM shutdown.
                        if (f.exists())
                        {
                            logger.warn(""Discovered obsolete snapshot. Deleting directory [{}]"", snapshotDirectory);
                            FileUtils.deleteRecursive(new File(snapshotDirectory));
                        }
                    }
                }

                // Only delete the old .toDelete file if we succeed in deleting all our known bad snapshots.
                Files.delete(Paths.get(TODELETEFILE));
            }
            catch (IOException e)
            {
                logger.warn(""Failed to open {}. Obsolete snapshots from previous runs will not be deleted."", TODELETEFILE, e);
            }
        }

        try
        {
            _failedSnapshotFile = new PrintWriter(new FileWriter(TODELETEFILE, true));
        }
        catch (IOException e)
        {
            throw new RuntimeException(String.format(""Failed to create failed snapshot tracking file [%s]. Aborting"", TODELETEFILE));
        }
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(O)org.apache.cassandra.service.CassandraDaemon:maybeInitJmx(),maybeInitJmx,CassandraDaemon,../data/xml/cassandra_call_methods/CassandraDaemon.xml,"
private void maybeInitJmx()
    {
        // If the standard com.sun.management.jmxremote.port property has been set
        // then the JVM agent will have already started up a default JMX connector
        // server. This behaviour is deprecated, but some clients may be relying
        // on it, so log a warning and skip setting up the server with the settings
        // as configured in cassandra-env.(sh|ps1)
        // See: CASSANDRA-11540 & CASSANDRA-11725
        if (COM_SUN_MANAGEMENT_JMXREMOTE_PORT.isPresent())
        {
            logger.warn(""JMX settings in cassandra-env.sh have been bypassed as the JMX connector server is "" +
                        ""already initialized. Please refer to cassandra-env.(sh|ps1) for JMX configuration info"");
            return;
        }

        System.setProperty(""java.rmi.server.randomIDs"", ""true"");

        // If a remote port has been specified then use that to set up a JMX
        // connector server which can be accessed remotely. Otherwise, look
        // for the local port property and create a server which is bound
        // only to the loopback address. Auth options are applied to both
        // remote and local-only servers, but currently SSL is only
        // available for remote.
        // If neither is remote nor local port is set in cassandra-env.(sh|ps)
        // then JMX is effectively  disabled.
        boolean localOnly = false;
        String jmxPort = CASSANDRA_JMX_REMOTE_PORT.getString();

        if (jmxPort == null)
        {
            localOnly = true;
            jmxPort = System.getProperty(""cassandra.jmx.local.port"");
        }

        if (jmxPort == null)
            return;

        try
        {
            jmxServer = JMXServerUtils.createJMXServer(Integer.parseInt(jmxPort), localOnly);
            if (jmxServer == null)
                return;
        }
        catch (IOException e)
        {
            exitOrFail(1, e.getMessage(), e.getCause());
        }
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(S)org.apache.cassandra.utils.Mx4jTool:maybeLoad(),maybeLoad,Mx4jTool,../data/xml/cassandra_call_methods/Mx4jTool.xml,"/**
     * Starts a JMX over http interface if and mx4j-tools.jar is in the classpath.
     * @return true if successfully loaded.
     */
public static boolean maybeLoad()
    {
        try
        {
            logger.trace(""Will try to load mx4j now, if it's in the classpath"");
            MBeanWrapper mbs = MBeanWrapper.instance;
            ObjectName processorName = new ObjectName(""Server:name=XSLTProcessor"");

            Class<?> httpAdaptorClass = Class.forName(""mx4j.tools.adaptor.http.HttpAdaptor"");
            Object httpAdaptor = httpAdaptorClass.newInstance();
            httpAdaptorClass.getMethod(""setHost"", String.class).invoke(httpAdaptor, getAddress());
            httpAdaptorClass.getMethod(""setPort"", Integer.TYPE).invoke(httpAdaptor, getPort());

            ObjectName httpName = new ObjectName(""system:name=http"");
            mbs.registerMBean(httpAdaptor, httpName);

            Class<?> xsltProcessorClass = Class.forName(""mx4j.tools.adaptor.http.XSLTProcessor"");
            Object xsltProcessor = xsltProcessorClass.newInstance();
            httpAdaptorClass.getMethod(""setProcessor"", Class.forName(""mx4j.tools.adaptor.http.ProcessorMBean"")).
                    invoke(httpAdaptor, xsltProcessor);
            mbs.registerMBean(xsltProcessor, processorName);
            httpAdaptorClass.getMethod(""start"").invoke(httpAdaptor);
            logger.info(""mx4j successfuly loaded"");
            return true;
        }
        catch (ClassNotFoundException e)
        {
            logger.trace(""Will not load MX4J, mx4j-tools.jar is not in the classpath"");
        }
        catch(Exception e)
        {
            logger.warn(""Could not start register mbean in JMX"", e);
        }
        return false;
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(S)org.apache.cassandra.security.ThreadAwareSecurityManager:install(),install,ThreadAwareSecurityManager,../data/xml/cassandra_call_methods/ThreadAwareSecurityManager.xml,"
public static void install()
    {
        if (installed)
            return;
        System.setSecurityManager(new ThreadAwareSecurityManager());
        LoggingSupportFactory.getLoggingSupport().onStartup();
        installed = true;
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(O)org.apache.cassandra.service.CassandraDaemon:logSystemInfo(),logSystemInfo,CassandraDaemon,../data/xml/cassandra_call_methods/CassandraDaemon.xml,"
private void logSystemInfo()
    {
    	if (logger.isInfoEnabled())
    	{
	        try
	        {
	            logger.info(""Hostname: {}"", InetAddress.getLocalHost().getHostName() + "":"" + DatabaseDescriptor.getStoragePort() + "":"" + DatabaseDescriptor.getSSLStoragePort());
	        }
	        catch (UnknownHostException e1)
	        {
	            logger.info(""Could not resolve local host"");
	        }

	        logger.info(""JVM vendor/version: {}/{}"", JAVA_VM_NAME.getString(), JAVA_VERSION.getString());
	        logger.info(""Heap size: {}/{}"",
                        FBUtilities.prettyPrintMemory(Runtime.getRuntime().totalMemory()),
                        FBUtilities.prettyPrintMemory(Runtime.getRuntime().maxMemory()));

	        for(MemoryPoolMXBean pool: ManagementFactory.getMemoryPoolMXBeans())
	            logger.info(""{} {}: {}"", pool.getName(), pool.getType(), pool.getPeakUsage());

	        logger.info(""Classpath: {}"", JAVA_CLASS_PATH.getString());

            logger.info(""JVM Arguments: {}"", ManagementFactory.getRuntimeMXBean().getInputArguments());
    	}
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(S)org.apache.cassandra.utils.NativeLibrary:tryMlockall(),tryMlockall,NativeLibrary,../data/xml/cassandra_call_methods/NativeLibrary.xml,"
public static void tryMlockall()
    {
        try
        {
            wrappedLibrary.callMlockall(MCL_CURRENT);
            jnaLockable = true;
            logger.info(""JNA mlockall successful"");
        }
        catch (UnsatisfiedLinkError e)
        {
            // this will have already been logged by CLibrary, no need to repeat it
        }
        catch (RuntimeException e)
        {
            if (!(e instanceof LastErrorException))
                throw e;

            if (errno(e) == ENOMEM && osType == LINUX)
            {
                logger.warn(""Unable to lock JVM memory (ENOMEM).""
                        + "" This can result in part of the JVM being swapped out, especially with mmapped I/O enabled.""
                        + "" Increase RLIMIT_MEMLOCK."");
            }
            else if (osType != MAC)
            {
                // OS X allows mlockall to be called, but always returns an error
                logger.warn(""Unknown mlockall error {}"", errno(e));
            }
        }
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(M)org.apache.cassandra.db.commitlog.CommitLog:start(),start,CommitLog,../data/xml/cassandra_call_methods/CommitLog.xml,"/**
     * Tries to start the CommitLog if not already started.
     */
synchronized public CommitLog start()
    {
        if (started)
            return this;

        try
        {
            segmentManager.start();
            executor.start();
            started = true;
        } catch (Throwable t)
        {
            started = false;
            throw t;
        }
        return this;
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(M)org.apache.cassandra.service.CassandraDaemon:runStartupChecks(),runStartupChecks,CassandraDaemon,../data/xml/cassandra_call_methods/CassandraDaemon.xml,"
public void runStartupChecks()
    {
        try
        {
            startupChecks.verify();
        }
        catch (StartupException e)
        {
            exitOrFail(e.returnCode, e.getMessage(), e.getCause());
        }

    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(S)org.apache.cassandra.db.SystemKeyspace:snapshotOnVersionChange(),snapshotOnVersionChange,SystemKeyspace,../data/xml/cassandra_call_methods/SystemKeyspace.xml,"/**
     * Compare the release version in the system.local table with the one included in the distro.
     * If they don't match, snapshot all tables in the system and schema keyspaces. This is intended
     * to be called at startup to create a backup of the system tables during an upgrade
     *
     * @throws IOException
     */
public static void snapshotOnVersionChange() throws IOException
    {
        String previous = getPreviousVersionString();
        String next = FBUtilities.getReleaseVersionString();

        FBUtilities.setPreviousReleaseVersionString(previous);

        // if we're restarting after an upgrade, snapshot the system and schema keyspaces
        if (!previous.equals(NULL_VERSION.toString()) && !previous.equals(next))

        {
            logger.info(""Detected version upgrade from {} to {}, snapshotting system keyspaces"", previous, next);
            String snapshotName = Keyspace.getTimestampedSnapshotName(format(""upgrade-%s-%s"",
                                                                             previous,
                                                                             next));
            for (String keyspace : SchemaConstants.LOCAL_SYSTEM_KEYSPACE_NAMES)
                Keyspace.open(keyspace).snapshot(snapshotName, null, false, null);
        }
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(S)org.apache.cassandra.db.SystemKeyspace:persistLocalMetadata(),persistLocalMetadata,SystemKeyspace,../data/xml/cassandra_call_methods/SystemKeyspace.xml,"
public static void persistLocalMetadata()
    {
        String req = ""INSERT INTO system.%s ("" +
                     ""key,"" +
                     ""cluster_name,"" +
                     ""release_version,"" +
                     ""cql_version,"" +
                     ""native_protocol_version,"" +
                     ""data_center,"" +
                     ""rack,"" +
                     ""partitioner,"" +
                     ""rpc_address,"" +
                     ""rpc_port,"" +
                     ""broadcast_address,"" +
                     ""broadcast_port,"" +
                     ""listen_address,"" +
                     ""listen_port"" +
                     "") VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)"";
        IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();
        executeOnceInternal(format(req, LOCAL),
                            LOCAL,
                            DatabaseDescriptor.getClusterName(),
                            FBUtilities.getReleaseVersionString(),
                            QueryProcessor.CQL_VERSION.toString(),
                            String.valueOf(ProtocolVersion.CURRENT.asInt()),
                            snitch.getLocalDatacenter(),
                            snitch.getLocalRack(),
                            DatabaseDescriptor.getPartitioner().getClass().getName(),
                            DatabaseDescriptor.getRpcAddress(),
                            DatabaseDescriptor.getNativeTransportPort(),
                            FBUtilities.getJustBroadcastAddress(),
                            DatabaseDescriptor.getStoragePort(),
                            FBUtilities.getJustLocalAddress(),
                            DatabaseDescriptor.getStoragePort());
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(S)org.apache.cassandra.db.SystemKeyspaceMigrator40:migrate(),migrate,SystemKeyspaceMigrator40,../data/xml/cassandra_call_methods/SystemKeyspaceMigrator40.xml,"
public static void migrate()
    {
        migratePeers();
        migratePeerEvents();
        migrateTransferredRanges();
        migrateAvailableRanges();
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(M)org.apache.cassandra.service.StorageService:populateTokenMetadata(),populateTokenMetadata,StorageService,../data/xml/cassandra_call_methods/StorageService.xml,"
public void populateTokenMetadata()
    {
        if (Boolean.parseBoolean(System.getProperty(""cassandra.load_ring_state"", ""true"")))
        {
            populatePeerTokenMetadata();
            // if we have not completed bootstrapping, we should not add ourselves as a normal token
            if (!shouldBootstrap())
                tokenMetadata.updateNormalTokens(SystemKeyspace.getSavedTokens(), FBUtilities.getBroadcastAddressAndPort());

            logger.info(""Token metadata: {}"", tokenMetadata);
        }
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(M)org.apache.cassandra.schema.Schema:loadFromDisk(),loadFromDisk,Schema,../data/xml/cassandra_call_methods/Schema.xml,"/**
     * load keyspace (keyspace) definitions, but do not initialize the keyspace instances.
     * Schema version may be updated as the result.
     */
public void loadFromDisk()
    {
        loadFromDisk(true);
    }

    
/**
     * Load schema definitions from disk.
     *
     * @param updateVersion true if schema version needs to be updated
     */
public void loadFromDisk(boolean updateVersion)
    {
        SchemaDiagnostics.schemataLoading(this);
        SchemaKeyspace.fetchNonSystemKeyspaces().forEach(this::load);
        if (updateVersion)
            updateVersion();
        SchemaDiagnostics.schemataLoaded(this);
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(M)org.apache.cassandra.service.CassandraDaemon:setupVirtualKeyspaces(),setupVirtualKeyspaces,CassandraDaemon,../data/xml/cassandra_call_methods/CassandraDaemon.xml,"
public void setupVirtualKeyspaces()
    {
        VirtualKeyspaceRegistry.instance.register(VirtualSchemaKeyspace.instance);
        VirtualKeyspaceRegistry.instance.register(SystemViewsKeyspace.instance);
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(S)org.apache.cassandra.io.sstable.SSTableHeaderFix:fixNonFrozenUDTIfUpgradeFrom30(),fixNonFrozenUDTIfUpgradeFrom30,SSTableHeaderFix,../data/xml/cassandra_call_methods/SSTableHeaderFix.xml,"
public static void fixNonFrozenUDTIfUpgradeFrom30()
    {
        String previousVersionString = FBUtilities.getPreviousReleaseVersionString();
        if (previousVersionString == null)
            return;
        CassandraVersion previousVersion = new CassandraVersion(previousVersionString);
        if (previousVersion.major != 3 || previousVersion.minor > 0)
        {
            // Not an upgrade from 3.0 to 3.x, nothing to do here
            return;
        }

        if (SKIP_AUTOMATIC_FIX_ON_UPGRADE)
        {
            logger.warn(""Detected upgrade from {} to {}, but -D{}=true, NOT fixing UDT type references in "" +
                        ""sstable metadata serialization-headers"",
                        previousVersionString,
                        FBUtilities.getReleaseVersionString(),
                        SKIPAUTOMATICUDTFIX);
            return;
        }

        logger.info(""Detected upgrade from {} to {}, fixing UDT type references in sstable metadata serialization-headers"",
                    previousVersionString,
                    FBUtilities.getReleaseVersionString());

        SSTableHeaderFix instance = SSTableHeaderFix.builder()
                                                    .schemaCallback(() -> Schema.instance::getTableMetadata)
                                                    .build();
        instance.execute();
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(M)org.apache.cassandra.schema.Schema:getKeyspaces(),getKeyspaces,Schema,../data/xml/cassandra_call_methods/Schema.xml,"/**
     * @return collection of the all keyspace names registered in the system (system and non-system)
     */
public Set<String> getKeyspaces()
    {
        return keyspaces.names();
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(S)org.apache.cassandra.db.ColumnFamilyStore:scrubDataDirectories(org.apache.cassandra.schema.TableMetadata),scrubDataDirectories,ColumnFamilyStore,../data/xml/cassandra_call_methods/ColumnFamilyStore.xml,"/**
     * Removes unnecessary files from the cf directory at startup: these include temp files, orphans, zero-length files
     * and compacted sstables. Files that cannot be recognized will be ignored.
     */
public static void  scrubDataDirectories(TableMetadata metadata) throws StartupException
    {
        Directories directories = new Directories(metadata);
        Set<File> cleanedDirectories = new HashSet<>();

        // clear ephemeral snapshots that were not properly cleared last session (CASSANDRA-7357)
        clearEphemeralSnapshots(directories);

        directories.removeTemporaryDirectories();

        logger.trace(""Removing temporary or obsoleted files from unfinished operations for table {}"", metadata.name);
        if (!LifecycleTransaction.removeUnfinishedLeftovers(metadata))
            throw new StartupException(StartupException.ERR_WRONG_DISK_STATE,
                                       String.format(""Cannot remove temporary or obsoleted files for %s due to a problem with transaction "" +
                                                     ""log files. Please check records with problems in the log messages above and fix them. "" +
                                                     ""Refer to the 3.0 upgrading instructions in NEWS.txt "" +
                                                     ""for a description of transaction log files."", metadata.toString()));

        logger.trace(""Further extra check for orphan sstable files for {}"", metadata.name);
        for (Map.Entry<Descriptor,Set<Component>> sstableFiles : directories.sstableLister(Directories.OnTxnErr.IGNORE).list().entrySet())
        {
            Descriptor desc = sstableFiles.getKey();
            File directory = desc.directory;
            Set<Component> components = sstableFiles.getValue();

            if (!cleanedDirectories.contains(directory))
            {
                cleanedDirectories.add(directory);
                for (File tmpFile : desc.getTemporaryFiles())
                {
                    logger.info(""Removing unfinished temporary file {}"", tmpFile);
                    tmpFile.delete();
                }
            }

            File dataFile = new File(desc.filenameFor(Component.DATA));
            if (components.contains(Component.DATA) && dataFile.length() > 0)
                // everything appears to be in order... moving on.
                continue;

            // missing the DATA file! all components are orphaned
            logger.warn(""Removing orphans for {}: {}"", desc, components);
            for (Component component : components)
            {
                File file = new File(desc.filenameFor(component));
                if (file.exists())
                    FileUtils.deleteWithConfirm(desc.filenameFor(component));
            }
        }

        // cleanup incomplete saved caches
        Pattern tmpCacheFilePattern = Pattern.compile(metadata.keyspace + '-' + metadata.name + ""-(Key|Row)Cache.*\\.tmp$"");
        File dir = new File(DatabaseDescriptor.getSavedCachesLocation());

        if (dir.exists())
        {
            assert dir.isDirectory();
            for (File file : Objects.requireNonNull(dir.listFiles()))
                if (tmpCacheFilePattern.matcher(file.getName()).matches())
                    if (!file.delete())
                        logger.warn(""could not delete {}"", file.getAbsolutePath());
        }

        // also clean out any index leftovers.
        for (IndexMetadata index : metadata.indexes)
            if (!index.isCustom())
            {
                TableMetadata indexMetadata = CassandraIndex.indexCfsMetadata(metadata, index);
                scrubDataDirectories(indexMetadata);
            }
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(M)org.apache.cassandra.exceptions.StartupException:getMessage(),getMessage,StartupException,../data/xml/cassandra_call_methods/StartupException.xml,not found
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(M)org.apache.cassandra.exceptions.StartupException:getCause(),getCause,StartupException,../data/xml/cassandra_call_methods/StartupException.xml,not found
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(S)org.apache.cassandra.db.Keyspace:setInitialized(),setInitialized,Keyspace,../data/xml/cassandra_call_methods/Keyspace.xml,"
public static void setInitialized()
    {
        initialized = true;
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(M)org.apache.cassandra.db.Keyspace:getColumnFamilyStores(),getColumnFamilyStores,Keyspace,../data/xml/cassandra_call_methods/Keyspace.xml,"
public Collection<ColumnFamilyStore> getColumnFamilyStores()
    {
        return Collections.unmodifiableCollection(columnFamilyStores.values());
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(M)org.apache.cassandra.db.ColumnFamilyStore:concatWithIndexes(),concatWithIndexes,ColumnFamilyStore,../data/xml/cassandra_call_methods/ColumnFamilyStore.xml,"
public Iterable<ColumnFamilyStore> concatWithIndexes()
    {
        // we return the main CFS first, which we rely on for simplicity in switchMemtable(), for getting the
        // latest commit log segment position
        return Iterables.concat(Collections.singleton(this), indexManager.getAllIndexColumnFamilyStores());
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(M)org.apache.cassandra.db.ColumnFamilyStore:disableAutoCompaction(),disableAutoCompaction,ColumnFamilyStore,../data/xml/cassandra_call_methods/ColumnFamilyStore.xml,"
public void disableAutoCompaction()
    {
        // we don't use CompactionStrategy.pause since we don't want users flipping that on and off
        // during runWithCompactionsDisabled
        compactionStrategyManager.disable();
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(O)org.apache.cassandra.service.CassandraDaemon:loadRowAndKeyCacheAsync(),loadRowAndKeyCacheAsync,CassandraDaemon,../data/xml/cassandra_call_methods/CassandraDaemon.xml,"/*
     * Asynchronously load the row and key cache in one off threads and return a compound future of the result.
     * Error handling is pushed into the cache load since cache loads are allowed to fail and are handled by logging.
     */
private ListenableFuture<?> loadRowAndKeyCacheAsync()
    {
        final ListenableFuture<Integer> keyCacheLoad = CacheService.instance.keyCache.loadSavedAsync();

        final ListenableFuture<Integer> rowCacheLoad = CacheService.instance.rowCache.loadSavedAsync();

        @SuppressWarnings(""unchecked"")
        ListenableFuture<List<Integer>> retval = Futures.successfulAsList(keyCacheLoad, rowCacheLoad);

        return retval;
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(S)org.apache.cassandra.service.GCInspector:register(),register,GCInspector,../data/xml/cassandra_call_methods/GCInspector.xml,"
public static void register() throws Exception
    {
        GCInspector inspector = new GCInspector();
        MBeanServer server = ManagementFactory.getPlatformMBeanServer();
        ObjectName gcName = new ObjectName(ManagementFactory.GARBAGE_COLLECTOR_MXBEAN_DOMAIN_TYPE + "",*"");
        for (ObjectName name : server.queryNames(gcName, null))
        {
            server.addNotificationListener(name, inspector, null, null);
        }
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(M)org.apache.cassandra.db.commitlog.CommitLog:recoverSegmentsOnDisk(),recoverSegmentsOnDisk,CommitLog,../data/xml/cassandra_call_methods/CommitLog.xml,"/**
     * Perform recovery on commit logs located in the directory specified by the config file.
     *
     * @return the number of mutations replayed
     * @throws IOException
     */
public int recoverSegmentsOnDisk() throws IOException
    {
        FilenameFilter unmanagedFilesFilter = (dir, name) -> CommitLogDescriptor.isValid(name) && CommitLogSegment.shouldReplay(name);

        // submit all files for this segment manager for archiving prior to recovery - CASSANDRA-6904
        // The files may have already been archived by normal CommitLog operation. This may cause errors in this
        // archiving pass, which we should not treat as serious.
        for (File file : new File(segmentManager.storageDirectory).listFiles(unmanagedFilesFilter))
        {
            archiver.maybeArchive(file.getPath(), file.getName());
            archiver.maybeWaitForArchiving(file.getName());
        }

        assert archiver.archivePending.isEmpty() : ""Not all commit log archive tasks were completed before restore"";
        archiver.maybeRestoreArchive();

        // List the files again as archiver may have added segments.
        File[] files = new File(segmentManager.storageDirectory).listFiles(unmanagedFilesFilter);
        int replayed = 0;
        if (files.length == 0)
        {
            logger.info(""No commitlog files found; skipping replay"");
        }
        else
        {
            Arrays.sort(files, new CommitLogSegmentFileComparator());
            logger.info(""Replaying {}"", StringUtils.join(files, "", ""));
            replayed = recoverFiles(files);
            logger.info(""Log replay complete, {} replayed mutations"", replayed);

            for (File f : files)
                segmentManager.handleReplayedSegment(f);
        }

        return replayed;
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(S)org.apache.cassandra.db.SystemKeyspace:finishStartup(),finishStartup,SystemKeyspace,../data/xml/cassandra_call_methods/SystemKeyspace.xml,"
public static void finishStartup()
    {
        Schema.instance.saveSystemKeyspace();
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(M)org.apache.cassandra.service.StorageService:cleanupSizeEstimates(),cleanupSizeEstimates,StorageService,../data/xml/cassandra_call_methods/StorageService.xml,"
public void cleanupSizeEstimates()
    {
        SystemKeyspace.clearAllEstimates();
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(M)org.apache.cassandra.service.ActiveRepairService:start(),start,ActiveRepairService,../data/xml/cassandra_call_methods/ActiveRepairService.xml,"
public void start()
    {
        consistent.local.start();
        ScheduledExecutors.optionalTasks.scheduleAtFixedRate(consistent.local::cleanup, 0,
                                                             LocalSessions.CLEANUP_INTERVAL,
                                                             TimeUnit.SECONDS);
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(M)org.apache.cassandra.cql3.QueryProcessor:preloadPreparedStatements(),preloadPreparedStatements,QueryProcessor,../data/xml/cassandra_call_methods/QueryProcessor.xml,"
public void preloadPreparedStatements()
    {
        int count = SystemKeyspace.loadPreparedStatements((id, query, keyspace) -> {
            try
            {
                ClientState clientState = ClientState.forInternalCalls();
                if (keyspace != null)
                    clientState.setKeyspace(keyspace);

                Prepared prepared = parseAndPrepare(query, clientState, false);
                preparedStatements.put(id, prepared);

                // Preload `null` statement for non-fully qualified statements, since it can't be parsed if loaded from cache and will be dropped
                if (!prepared.fullyQualified)
                    preparedStatements.get(computeId(query, null), (ignored_) -> prepared);
                return true;
            }
            catch (RequestValidationException e)
            {
                JVMStabilityInspector.inspectThrowable(e);
                logger.warn(String.format(""Prepared statement recreation error, removing statement: %s %s %s"", id, query, keyspace));
                SystemKeyspace.removePreparedStatement(id);
                return false;
            }
        });
        logger.info(""Preloaded {} prepared statements"", count);
    }


    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(M)org.apache.cassandra.service.StorageService:registerDaemon(org.apache.cassandra.service.CassandraDaemon),registerDaemon,StorageService,../data/xml/cassandra_call_methods/StorageService.xml,"
public void registerDaemon(CassandraDaemon daemon)
    {
        this.daemon = daemon;
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(M)org.apache.cassandra.service.StorageService:initServer(),initServer,StorageService,../data/xml/cassandra_call_methods/StorageService.xml,"
public synchronized void initServer() throws ConfigurationException
    {
        initServer(RING_DELAY);
    }

    

public synchronized void initServer(int delay) throws ConfigurationException
    {
        logger.info(""Cassandra version: {}"", FBUtilities.getReleaseVersionString());
        logger.info(""CQL version: {}"", QueryProcessor.CQL_VERSION);
        logger.info(""Native protocol supported versions: {} (default: {})"",
                    StringUtils.join(ProtocolVersion.supportedVersions(), "", ""), ProtocolVersion.CURRENT);

        try
        {
            // Ensure StorageProxy is initialized on start-up; see CASSANDRA-3797.
            Class.forName(""org.apache.cassandra.service.StorageProxy"");
            // also IndexSummaryManager, which is otherwise unreferenced
            Class.forName(""org.apache.cassandra.io.sstable.IndexSummaryManager"");
        }
        catch (ClassNotFoundException e)
        {
            throw new AssertionError(e);
        }

        if (Boolean.parseBoolean(System.getProperty(""cassandra.load_ring_state"", ""true"")))
        {
            logger.info(""Loading persisted ring state"");
            populatePeerTokenMetadata();
            for (InetAddressAndPort endpoint : tokenMetadata.getAllEndpoints())
                Gossiper.runInGossipStageBlocking(() -> Gossiper.instance.addSavedEndpoint(endpoint));
        }

        // daemon threads, like our executors', continue to run while shutdown hooks are invoked
        drainOnShutdown = NamedThreadFactory.createThread(new WrappedRunnable()
        {
            @Override
            public void runMayThrow() throws InterruptedException, ExecutionException, IOException
            {
                drain(true);

                if (FBUtilities.isWindows)
                    WindowsTimer.endTimerPeriod(DatabaseDescriptor.getWindowsTimerInterval());

                LoggingSupportFactory.getLoggingSupport().onShutdown();
            }
        }, ""StorageServiceShutdownHook"");
        Runtime.getRuntime().addShutdownHook(drainOnShutdown);

        replacing = isReplacing();

        if (!Boolean.parseBoolean(System.getProperty(""cassandra.start_gossip"", ""true"")))
        {
            logger.info(""Not starting gossip as requested."");
            initialized = true;
            return;
        }

        prepareToJoin();

        // Has to be called after the host id has potentially changed in prepareToJoin().
        try
        {
            CacheService.instance.counterCache.loadSavedAsync().get();
        }
        catch (Throwable t)
        {
            JVMStabilityInspector.inspectThrowable(t);
            logger.warn(""Error loading counter cache"", t);
        }

        if (joinRing)
        {
            joinTokenRing(delay);
        }
        else
        {
            Collection<Token> tokens = SystemKeyspace.getSavedTokens();
            if (!tokens.isEmpty())
            {
                tokenMetadata.updateNormalTokens(tokens, FBUtilities.getBroadcastAddressAndPort());
                // order is important here, the gossiper can fire in between adding these two states.  It's ok to send TOKENS without STATUS, but *not* vice versa.
                List<Pair<ApplicationState, VersionedValue>> states = new ArrayList<Pair<ApplicationState, VersionedValue>>();
                states.add(Pair.create(ApplicationState.TOKENS, valueFactory.tokens(tokens)));
                states.add(Pair.create(ApplicationState.STATUS_WITH_PORT, valueFactory.hibernate(true)));
                states.add(Pair.create(ApplicationState.STATUS, valueFactory.hibernate(true)));
                Gossiper.instance.addLocalApplicationStates(states);
            }
            doAuthSetup(true);
            logger.info(""Not joining ring as requested. Use JMX (StorageService->joinRing()) to initiate ring joining"");
        }

        initialized = true;
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(M)org.apache.cassandra.exceptions.ConfigurationException:getMessage(),getMessage,ConfigurationException,../data/xml/cassandra_call_methods/ConfigurationException.xml,not found
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(S)org.apache.cassandra.utils.FBUtilities:getBroadcastAddressAndPort(),getBroadcastAddressAndPort,FBUtilities,../data/xml/cassandra_call_methods/FBUtilities.xml,"/**
     * Get the broadcast address and port for intra-cluster storage traffic. This the address to advertise that uniquely
     * identifies the node and is reachable from everywhere. This is the one you want unless you are trying to connect
     * to the local address specifically.
     */
public static InetAddressAndPort getBroadcastAddressAndPort()
    {
        if (broadcastInetAddressAndPort == null)
        {
            if(DatabaseDescriptor.getRawConfig() == null)
            {
                broadcastInetAddressAndPort = InetAddressAndPort.getByAddress(getJustBroadcastAddress());
            }
            else
            {
                broadcastInetAddressAndPort = InetAddressAndPort.getByAddressOverrideDefaults(getJustBroadcastAddress(),
                                                                                              DatabaseDescriptor.getStoragePort());
            }
        }
        return broadcastInetAddressAndPort;
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(S)org.apache.cassandra.locator.InetAddressAndPort:getLoopbackAddress(),getLoopbackAddress,InetAddressAndPort,../data/xml/cassandra_call_methods/InetAddressAndPort.xml,"
public static InetAddressAndPort getLoopbackAddress()
    {
        return InetAddressAndPort.getByAddress(InetAddress.getLoopbackAddress());
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(S)org.apache.cassandra.gms.Gossiper:waitToSettle(),waitToSettle,Gossiper,../data/xml/cassandra_call_methods/Gossiper.xml,"
public static void waitToSettle()
    {
        int forceAfter = Integer.getInteger(""cassandra.skip_wait_for_gossip_to_settle"", -1);
        if (forceAfter == 0)
        {
            return;
        }
        final int GOSSIP_SETTLE_MIN_WAIT_MS = 5000;
        final int GOSSIP_SETTLE_POLL_INTERVAL_MS = 1000;
        final int GOSSIP_SETTLE_POLL_SUCCESSES_REQUIRED = 3;

        logger.info(""Waiting for gossip to settle..."");
        Uninterruptibles.sleepUninterruptibly(GOSSIP_SETTLE_MIN_WAIT_MS, TimeUnit.MILLISECONDS);
        int totalPolls = 0;
        int numOkay = 0;
        int epSize = Gossiper.instance.getEndpointCount();
        while (numOkay < GOSSIP_SETTLE_POLL_SUCCESSES_REQUIRED)
        {
            Uninterruptibles.sleepUninterruptibly(GOSSIP_SETTLE_POLL_INTERVAL_MS, TimeUnit.MILLISECONDS);
            int currentSize = Gossiper.instance.getEndpointCount();
            totalPolls++;
            if (currentSize == epSize)
            {
                logger.debug(""Gossip looks settled."");
                numOkay++;
            }
            else
            {
                logger.info(""Gossip not settled after {} polls."", totalPolls);
                numOkay = 0;
            }
            epSize = currentSize;
            if (forceAfter > 0 && totalPolls > forceAfter)
            {
                logger.warn(""Gossip not settled but startup forced by cassandra.skip_wait_for_gossip_to_settle. Gossip total polls: {}"",
                            totalPolls);
                break;
            }
        }
        if (totalPolls > GOSSIP_SETTLE_POLL_SUCCESSES_REQUIRED)
            logger.info(""Gossip settled after {} extra polls; proceeding"", totalPolls - GOSSIP_SETTLE_POLL_SUCCESSES_REQUIRED);
        else
            logger.info(""No gossip backlog; proceeding"");
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(M)org.apache.cassandra.service.StorageService:doAuthSetup(boolean),doAuthSetup,StorageService,../data/xml/cassandra_call_methods/StorageService.xml,"
@VisibleForTesting
    public void doAuthSetup(boolean setUpSchema)
    {
        if (!authSetupCalled.getAndSet(true))
        {
            if (setUpSchema)
            {
                Optional<Mutation> mutation = evolveSystemKeyspace(AuthKeyspace.metadata(), AuthKeyspace.GENERATION);
                mutation.ifPresent(value -> FBUtilities.waitOnFuture(MigrationManager.announceWithoutPush(Collections.singleton(value))));
            }

            DatabaseDescriptor.getRoleManager().setup();
            DatabaseDescriptor.getAuthenticator().setup();
            DatabaseDescriptor.getAuthorizer().setup();
            DatabaseDescriptor.getNetworkAuthorizer().setup();
            Schema.instance.registerListener(new AuthSchemaChangeListener());
            authSetupComplete = true;
        }
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(S)org.apache.cassandra.db.Keyspace:all(),all,Keyspace,../data/xml/cassandra_call_methods/Keyspace.xml,"
public static Iterable<Keyspace> all()
    {
        return Iterables.transform(Schema.instance.getKeyspaces(), Keyspace::open);
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(M)org.apache.cassandra.db.ColumnFamilyStore:reload(),reload,ColumnFamilyStore,../data/xml/cassandra_call_methods/ColumnFamilyStore.xml,"
public void reload()
    {
        // metadata object has been mutated directly. make all the members jibe with new settings.

        // only update these runtime-modifiable settings if they have not been modified.
        if (!minCompactionThreshold.isModified())
            for (ColumnFamilyStore cfs : concatWithIndexes())
                cfs.minCompactionThreshold = new DefaultValue(metadata().params.compaction.minCompactionThreshold());
        if (!maxCompactionThreshold.isModified())
            for (ColumnFamilyStore cfs : concatWithIndexes())
                cfs.maxCompactionThreshold = new DefaultValue(metadata().params.compaction.maxCompactionThreshold());
        if (!crcCheckChance.isModified())
            for (ColumnFamilyStore cfs : concatWithIndexes())
                cfs.crcCheckChance = new DefaultValue(metadata().params.crcCheckChance);

        compactionStrategyManager.maybeReload(metadata());

        scheduleFlush();

        indexManager.reload();

        // If the CF comparator has changed, we need to change the memtable,
        // because the old one still aliases the previous comparator.
        if (data.getView().getCurrentMemtable().initialComparator != metadata().comparator)
            switchMemtable();
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(M)org.apache.cassandra.db.ColumnFamilyStore:getCompactionStrategyManager(),getCompactionStrategyManager,ColumnFamilyStore,../data/xml/cassandra_call_methods/ColumnFamilyStore.xml,"/*
     JMX getters and setters for the Default<T>s.
       - get/set minCompactionThreshold
       - get/set maxCompactionThreshold
       - get     memsize
       - get     memops
       - get/set memtime
     */
public CompactionStrategyManager getCompactionStrategyManager()
    {
        return compactionStrategyManager;
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(M)org.apache.cassandra.db.compaction.CompactionStrategyManager:shouldBeEnabled(),shouldBeEnabled,CompactionStrategyManager,../data/xml/cassandra_call_methods/CompactionStrategyManager.xml,"
public boolean shouldBeEnabled()
    {
        return params.isEnabled();
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(S)org.apache.cassandra.config.DatabaseDescriptor:getAutocompactionOnStartupEnabled(),getAutocompactionOnStartupEnabled,DatabaseDescriptor,../data/xml/cassandra_call_methods/DatabaseDescriptor.xml,"
public static boolean getAutocompactionOnStartupEnabled()
    {
        return conf.autocompaction_on_startup_enabled;
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(M)org.apache.cassandra.db.ColumnFamilyStore:enableAutoCompaction(),enableAutoCompaction,ColumnFamilyStore,../data/xml/cassandra_call_methods/ColumnFamilyStore.xml,"
public void enableAutoCompaction()
    {
        enableAutoCompaction(false);
    }

    
/**
     * used for tests - to be able to check things after a minor compaction
     * @param waitForFutures if we should block until autocompaction is done
     */
@VisibleForTesting
    public void enableAutoCompaction(boolean waitForFutures)
    {
        compactionStrategyManager.enable();
        List<Future<?>> futures = CompactionManager.instance.submitBackground(this);
        if (waitForFutures)
            FBUtilities.waitOnFutures(futures);
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(M)org.apache.cassandra.audit.AuditLogManager:initialize(),initialize,AuditLogManager,../data/xml/cassandra_call_methods/AuditLogManager.xml,"
public void initialize()
    {
        if (DatabaseDescriptor.getAuditLoggingOptions().enabled)
            registerAsListener();
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(S)org.apache.cassandra.db.ColumnFamilyStore:getBackgroundCompactionTaskSubmitter(),getBackgroundCompactionTaskSubmitter,ColumnFamilyStore,../data/xml/cassandra_call_methods/ColumnFamilyStore.xml,"
public static Runnable getBackgroundCompactionTaskSubmitter()
    {
        return () -> {
            for (Keyspace keyspace : Keyspace.all())
                for (ColumnFamilyStore cfs : keyspace.getColumnFamilyStores())
                    CompactionManager.instance.submitBackground(cfs);
        };
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(M)org.apache.cassandra.service.CassandraDaemon:initializeClientTransports(),initializeClientTransports,CassandraDaemon,../data/xml/cassandra_call_methods/CassandraDaemon.xml,"
public synchronized void initializeClientTransports()
    {
        // Native transport
        if (nativeTransportService == null)
            nativeTransportService = new NativeTransportService();
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(M)org.apache.cassandra.service.CassandraDaemon:completeSetup(),completeSetup,CassandraDaemon,../data/xml/cassandra_call_methods/CassandraDaemon.xml,"
@VisibleForTesting
    public void completeSetup()
    {
        setupCompleted = true;
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(M)org.apache.cassandra.service.reads.repair.BlockingPartitionRepair:blockFor(),blockFor,BlockingPartitionRepair,../data/xml/cassandra_call_methods/BlockingPartitionRepair.xml,"
int blockFor()
    {
        return writePlan.blockFor();
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(M)org.apache.cassandra.service.reads.repair.BlockingPartitionRepair:waitingOn(),waitingOn,BlockingPartitionRepair,../data/xml/cassandra_call_methods/BlockingPartitionRepair.xml,"
@VisibleForTesting
    int waitingOn()
    {
        return (int) latch.getCount();
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(S)org.apache.cassandra.tracing.Tracing:isTracing(),isTracing,Tracing,../data/xml/cassandra_call_methods/Tracing.xml,"/**
     * Indicates if the current thread's execution is being traced.
     */
public static boolean isTracing()
    {
        return instance.get() != null;
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(M)org.apache.cassandra.service.reads.repair.BlockingReadRepair:replicaPlan(),replicaPlan,BlockingReadRepair,../data/xml/cassandra_call_methods/BlockingReadRepair.xml,not found
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,(M)org.apache.cassandra.locator.ReplicaPlan$ForRead:consistencyLevel(),consistencyLevel,ReplicaPlan$ForRead,../data/xml/cassandra_call_methods/ReplicaPlan.xml,"
public ConsistencyLevel consistencyLevel() { return consistencyLevel; }

    "
org.apache.cassandra.config.DatabaseDescriptor:getReadRpcTimeout(java.util.concurrent.TimeUnit),read_request_timeout_in_ms,"(O)org.apache.cassandra.exceptions.ReadTimeoutException:<init>(org.apache.cassandra.db.ConsistencyLevel,int,int,boolean)",<init>,ReadTimeoutException,../data/xml/cassandra_call_methods/ReadTimeoutException.xml,"
public ReadTimeoutException(ConsistencyLevel consistency, int received, int blockFor, boolean dataPresent)
    {
        super(ExceptionCode.READ_TIMEOUT, consistency, received, blockFor);
        this.dataPresent = dataPresent;
    }
}"
