function,option,Method,Method_short,class_name,xml_path,Method_body
org.apache.cassandra.config.DatabaseDescriptor:getCachedReplicaRowsWarnThreshold(),cached_rows_warn_threshold,(S)org.apache.cassandra.config.DatabaseDescriptor:getCachedReplicaRowsWarnThreshold(),getCachedReplicaRowsWarnThreshold,DatabaseDescriptor,../data/xml/cassandra_call_methods/DatabaseDescriptor.xml,"
public static int getCachedReplicaRowsWarnThreshold()
    {
        return conf.replica_filtering_protection.cached_rows_warn_threshold;
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getCachedReplicaRowsWarnThreshold(),cached_rows_warn_threshold,"(O)org.apache.cassandra.service.reads.DataResolver$ResolveContext:<init>(org.apache.cassandra.service.reads.DataResolver,org.apache.cassandra.locator.Endpoints,org.apache.cassandra.service.reads.DataResolver$1)",<init>,DataResolver$ResolveContext,../data/xml/cassandra_call_methods/DataResolver.xml,"
DataResolver<E extends Endpoints<E>, P extends ReplicaPlan.ForRead<E>> 

public DataResolver(ReadCommand command, ReplicaPlan.Shared<E, P> replicaPlan, ReadRepair<E, P> readRepair, long queryStartNanoTime)
    {
        this(command, replicaPlan, readRepair, queryStartNanoTime, false);
    }

    

public DataResolver(ReadCommand command, ReplicaPlan.Shared<E, P> replicaPlan, ReadRepair<E, P> readRepair, long queryStartNanoTime, boolean trackRepairedStatus)
    {
        super(command, replicaPlan, queryStartNanoTime);
        this.enforceStrictLiveness = command.metadata().enforceStrictLiveness();
        this.readRepair = readRepair;
        this.trackRepairedStatus = trackRepairedStatus;
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getCachedReplicaRowsWarnThreshold(),cached_rows_warn_threshold,(M)org.apache.cassandra.service.reads.DataResolver:replicaPlan(),replicaPlan,DataResolver,../data/xml/cassandra_call_methods/DataResolver.xml,not found
org.apache.cassandra.config.DatabaseDescriptor:getCachedReplicaRowsWarnThreshold(),cached_rows_warn_threshold,(M)org.apache.cassandra.locator.ReplicaPlan$ForRead:keyspace(),keyspace,ReplicaPlan$ForRead,../data/xml/cassandra_call_methods/ReplicaPlan.xml,"
public Keyspace keyspace() { return keyspace; }
    "
org.apache.cassandra.config.DatabaseDescriptor:getCachedReplicaRowsWarnThreshold(),cached_rows_warn_threshold,(M)org.apache.cassandra.locator.ReplicaPlan$ForRead:consistencyLevel(),consistencyLevel,ReplicaPlan$ForRead,../data/xml/cassandra_call_methods/ReplicaPlan.xml,"
public ConsistencyLevel consistencyLevel() { return consistencyLevel; }

    "
org.apache.cassandra.config.DatabaseDescriptor:getCachedReplicaRowsWarnThreshold(),cached_rows_warn_threshold,(S)org.apache.cassandra.service.reads.DataResolver$ResolveContext:access$300(org.apache.cassandra.service.reads.DataResolver$ResolveContext),access$300,DataResolver$ResolveContext,../data/xml/cassandra_call_methods/DataResolver.xml,"
DataResolver<E extends Endpoints<E>, P extends ReplicaPlan.ForRead<E>> 

public DataResolver(ReadCommand command, ReplicaPlan.Shared<E, P> replicaPlan, ReadRepair<E, P> readRepair, long queryStartNanoTime)
    {
        this(command, replicaPlan, readRepair, queryStartNanoTime, false);
    }

    

public DataResolver(ReadCommand command, ReplicaPlan.Shared<E, P> replicaPlan, ReadRepair<E, P> readRepair, long queryStartNanoTime, boolean trackRepairedStatus)
    {
        super(command, replicaPlan, queryStartNanoTime);
        this.enforceStrictLiveness = command.metadata().enforceStrictLiveness();
        this.readRepair = readRepair;
        this.trackRepairedStatus = trackRepairedStatus;
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getCachedReplicaRowsWarnThreshold(),cached_rows_warn_threshold,(S)org.apache.cassandra.config.DatabaseDescriptor:getCachedReplicaRowsFailThreshold(),getCachedReplicaRowsFailThreshold,DatabaseDescriptor,../data/xml/cassandra_call_methods/DatabaseDescriptor.xml,"
public static int getCachedReplicaRowsFailThreshold()
    {
        return conf.replica_filtering_protection.cached_rows_fail_threshold;
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getCachedReplicaRowsWarnThreshold(),cached_rows_warn_threshold,"(O)org.apache.cassandra.service.reads.ReplicaFilteringProtection:<init>(org.apache.cassandra.db.Keyspace,org.apache.cassandra.db.ReadCommand,org.apache.cassandra.db.ConsistencyLevel,long,org.apache.cassandra.locator.Endpoints,int,int)",<init>,ReplicaFilteringProtection,../data/xml/cassandra_call_methods/ReplicaFilteringProtection.xml,"
ReplicaFilteringProtection(Keyspace keyspace,
                               ReadCommand command,
                               ConsistencyLevel consistency,
                               long queryStartNanoTime,
                               E sources,
                               int cachedRowsWarnThreshold,
                               int cachedRowsFailThreshold)
    {
        this.keyspace = keyspace;
        this.command = command;
        this.consistency = consistency;
        this.queryStartNanoTime = queryStartNanoTime;
        this.sources = sources;
        this.originalPartitions = new ArrayList<>(sources.size());

        for (int i = 0; i < sources.size(); i++)
        {
            originalPartitions.add(new ArrayDeque<>());
        }

        tableMetrics = ColumnFamilyStore.metricsFor(command.metadata().id);

        this.cachedRowsWarnThreshold = cachedRowsWarnThreshold;
        this.cachedRowsFailThreshold = cachedRowsFailThreshold;
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getCachedReplicaRowsWarnThreshold(),cached_rows_warn_threshold,(M)org.apache.cassandra.service.reads.ReplicaFilteringProtection:mergeController(),mergeController,ReplicaFilteringProtection,../data/xml/cassandra_call_methods/ReplicaFilteringProtection.xml,"/**
     * Returns a merge listener that skips the merged rows for which any of the replicas doesn't have a version,
     * pessimistically assuming that they are outdated. It is intended to be used during a first merge of per-replica
     * query results to ensure we fetch enough results from the replicas to ensure we don't miss any potentially
     * outdated result.
     * <p>
     * The listener will track both the accepted data and the primary keys of the rows that are considered as outdated.
     * That way, once the query results would have been merged using this listener, further calls to
     * {@link #queryProtectedPartitions(PartitionIterator, int)} will use the collected data to return a copy of the
     * data originally collected from the specified replica, completed with the potentially outdated rows.
     */
UnfilteredPartitionIterators.MergeListener mergeController()
    {
        return new UnfilteredPartitionIterators.MergeListener()
        {
            @Override
            public void close()
            {
                // If we hit the failure threshold before consuming a single partition, record the current rows cached.
                tableMetrics.rfpRowsCachedPerQuery.update(Math.max(currentRowsCached, maxRowsCached));
            }

            @Override
            public UnfilteredRowIterators.MergeListener getRowMergeListener(DecoratedKey partitionKey, List<UnfilteredRowIterator> versions)
            {
                List<PartitionBuilder> builders = new ArrayList<>(sources.size());
                RegularAndStaticColumns columns = columns(versions);
                EncodingStats stats = EncodingStats.merge(versions, NULL_TO_NO_STATS);

                for (int i = 0; i < sources.size(); i++)
                    builders.add(i, new PartitionBuilder(partitionKey, sources.get(i), columns, stats));

                return new UnfilteredRowIterators.MergeListener()
                {
                    @Override
                    public void onMergedPartitionLevelDeletion(DeletionTime mergedDeletion, DeletionTime[] versions)
                    {
                        // cache the deletion time versions to be able to regenerate the original row iterator
                        for (int i = 0; i < versions.length; i++)
                            builders.get(i).setDeletionTime(versions[i]);
                    }

                    @Override
                    public Row onMergedRows(Row merged, Row[] versions)
                    {
                        // cache the row versions to be able to regenerate the original row iterator
                        for (int i = 0; i < versions.length; i++)
                            builders.get(i).addRow(versions[i]);

                        if (merged.isEmpty())
                            return merged;

                        boolean isPotentiallyOutdated = false;
                        boolean isStatic = merged.isStatic();
                        for (int i = 0; i < versions.length; i++)
                        {
                            Row version = versions[i];
                            if (version == null || (isStatic && version.isEmpty()))
                            {
                                isPotentiallyOutdated = true;
                                builders.get(i).addToFetch(merged);
                            }
                        }

                        // If the row is potentially outdated (because some replica didn't send anything and so it _may_ be
                        // an outdated result that is only present because other replica have filtered the up-to-date result
                        // out), then we skip the row. In other words, the results of the initial merging of results by this
                        // protection assume the worst case scenario where every row that might be outdated actually is.
                        // This ensures that during this first phase (collecting additional row to fetch) we are guaranteed
                        // to look at enough data to ultimately fulfill the query limit.
                        return isPotentiallyOutdated ? null : merged;
                    }

                    @Override
                    public void onMergedRangeTombstoneMarkers(RangeTombstoneMarker merged, RangeTombstoneMarker[] versions)
                    {
                        // cache the marker versions to be able to regenerate the original row iterator
                        for (int i = 0; i < versions.length; i++)
                            builders.get(i).addRangeTombstoneMarker(versions[i]);
                    }

                    @Override
                    public void close()
                    {
                        for (int i = 0; i < sources.size(); i++)
                            originalPartitions.get(i).add(builders.get(i));
                    }
                };
            }
        };
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getCachedReplicaRowsWarnThreshold(),cached_rows_warn_threshold,"(D)org.apache.cassandra.service.reads.DataResolver$ResponseProvider:getResponse(org.apache.cassandra.service.reads.DataResolver,org.apache.cassandra.service.reads.DataResolver$ResolveContext)",getResponse,DataResolver$ResponseProvider,../data/xml/cassandra_call_methods/DataResolver.xml,not found
org.apache.cassandra.config.DatabaseDescriptor:getCachedReplicaRowsWarnThreshold(),cached_rows_warn_threshold,"(D)org.apache.cassandra.service.reads.DataResolver$ResponseProvider:getResponse(org.apache.cassandra.service.reads.ReplicaFilteringProtection,org.apache.cassandra.db.partitions.PartitionIterator)",getResponse,DataResolver$ResponseProvider,../data/xml/cassandra_call_methods/DataResolver.xml,not found
