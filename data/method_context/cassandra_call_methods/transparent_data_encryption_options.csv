function,option,Method,Method_short,class_name,xml_path,Method_body
org.apache.cassandra.config.DatabaseDescriptor:getEncryptionContext(),transparent_data_encryption_options,(S)org.apache.cassandra.config.DatabaseDescriptor:getCommitLogCompression(),getCommitLogCompression,DatabaseDescriptor,../data/xml/cassandra_call_methods/DatabaseDescriptor.xml,"
public static ParameterizedClass getCommitLogCompression()
    {
        return conf.commitlog_compression;
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getEncryptionContext(),transparent_data_encryption_options,(S)org.apache.cassandra.config.DatabaseDescriptor:getEncryptionContext(),getEncryptionContext,DatabaseDescriptor,../data/xml/cassandra_call_methods/DatabaseDescriptor.xml,"
public static EncryptionContext getEncryptionContext()
    {
        return encryptionContext;
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getEncryptionContext(),transparent_data_encryption_options,"(O)org.apache.cassandra.db.commitlog.CommitLog$Configuration:<init>(org.apache.cassandra.config.ParameterizedClass,org.apache.cassandra.security.EncryptionContext)",<init>,CommitLog$Configuration,../data/xml/cassandra_call_methods/CommitLog.xml,"/*
 * Commit Log tracks every write operation into the system. The aim of the commit log is to be able to
 * successfully recover data that was not stored to disk via the Memtable.
 */
public class CommitLog implements CommitLogMBean
{
    private static final Logger logger = LoggerFactory.getLogger(CommitLog.class);

    public static final CommitLog instance = CommitLog.construct();

    final public AbstractCommitLogSegmentManager segmentManager;

    public final CommitLogArchiver archiver;
    public final CommitLogMetrics metrics;
    final AbstractCommitLogService executor;

    volatile Configuration configuration;
    private boolean started = false;

    private static CommitLog construct()
    {
        CommitLog log = new CommitLog(CommitLogArchiver.construct(), DatabaseDescriptor.getCommitLogSegmentMgrProvider());

        MBeanWrapper.instance.registerMBean(log, ""org.apache.cassandra.db:type=Commitlog"");
        return log;
    }

    @VisibleForTesting
    CommitLog(CommitLogArchiver archiver)
    {
        this(archiver, DatabaseDescriptor.getCommitLogSegmentMgrProvider());
    }

    @VisibleForTesting
    CommitLog(CommitLogArchiver archiver, Function<CommitLog, AbstractCommitLogSegmentManager> segmentManagerProvider)
    {
        this.configuration = new Configuration(DatabaseDescriptor.getCommitLogCompression(),
                                               DatabaseDescriptor.getEncryptionContext());
        DatabaseDescriptor.createAllDirectories();

        this.archiver = archiver;
        metrics = new CommitLogMetrics();

        switch (DatabaseDescriptor.getCommitLogSync())
        {
            case periodic:
                executor = new PeriodicCommitLogService(this);
                break;
            case batch:
                executor = new BatchCommitLogService(this);
                break;
            case group:
                executor = new GroupCommitLogService(this);
                break;
            default:
                throw new IllegalArgumentException(""Unknown commitlog service type: "" + DatabaseDescriptor.getCommitLogSync());
        }

        segmentManager = segmentManagerProvider.apply(this);

        // register metrics
        metrics.attach(executor, segmentManager);
    }

    /**
     * Tries to start the CommitLog if not already started.
     */
    synchronized public CommitLog start()
    {
        if (started)
            return this;

        try
        {
            segmentManager.start();
            executor.start();
            started = true;
        } catch (Throwable t)
        {
            started = false;
            throw t;
        }
        return this;
    }

    /**
     * Perform recovery on commit logs located in the directory specified by the config file.
     *
     * @return the number of mutations replayed
     * @throws IOException
     */
    public int recoverSegmentsOnDisk() throws IOException
    {
        FilenameFilter unmanagedFilesFilter = (dir, name) -> CommitLogDescriptor.isValid(name) && CommitLogSegment.shouldReplay(name);

        // submit all files for this segment manager for archiving prior to recovery - CASSANDRA-6904
        // The files may have already been archived by normal CommitLog operation. This may cause errors in this
        // archiving pass, which we should not treat as serious.
        for (File file : new File(segmentManager.storageDirectory).listFiles(unmanagedFilesFilter))
        {
            archiver.maybeArchive(file.getPath(), file.getName());
            archiver.maybeWaitForArchiving(file.getName());
        }

        assert archiver.archivePending.isEmpty() : ""Not all commit log archive tasks were completed before restore"";
        archiver.maybeRestoreArchive();

        // List the files again as archiver may have added segments.
        File[] files = new File(segmentManager.storageDirectory).listFiles(unmanagedFilesFilter);
        int replayed = 0;
        if (files.length == 0)
        {
            logger.info(""No commitlog files found; skipping replay"");
        }
        else
        {
            Arrays.sort(files, new CommitLogSegmentFileComparator());
            logger.info(""Replaying {}"", StringUtils.join(files, "", ""));
            replayed = recoverFiles(files);
            logger.info(""Log replay complete, {} replayed mutations"", replayed);

            for (File f : files)
                segmentManager.handleReplayedSegment(f);
        }

        return replayed;
    }

    /**
     * Perform recovery on a list of commit log files.
     *
     * @param clogs   the list of commit log files to replay
     * @return the number of mutations replayed
     */
    public int recoverFiles(File... clogs) throws IOException
    {
        CommitLogReplayer replayer = CommitLogReplayer.construct(this, getLocalHostId());
        replayer.replayFiles(clogs);
        return replayer.blockForWrites();
    }

    public void recoverPath(String path) throws IOException
    {
        CommitLogReplayer replayer = CommitLogReplayer.construct(this, getLocalHostId());
        replayer.replayPath(new File(path), false);
        replayer.blockForWrites();
    }

    private static UUID getLocalHostId()
    {
        return Optional.ofNullable(StorageService.instance.getLocalHostUUID()).orElseGet(SystemKeyspace::getLocalHostId);
    }

    /**
     * Perform recovery on a single commit log. Kept w/sub-optimal name due to coupling w/MBean / JMX
     */
    public void recover(String path) throws IOException
    {
        recoverPath(path);
    }

    /**
     * @return a CommitLogPosition which, if {@code >= one} returned from add(), implies add() was started
     * (but not necessarily finished) prior to this call
     */
    public CommitLogPosition getCurrentPosition()
    {
        return segmentManager.getCurrentPosition();
    }

    /**
     * Flushes all dirty CFs, waiting for them to free and recycle any segments they were retaining
     */
    public void forceRecycleAllSegments(Iterable<TableId> droppedTables)
    {
        segmentManager.forceRecycleAll(droppedTables);
    }

    /**
     * Flushes all dirty CFs, waiting for them to free and recycle any segments they were retaining
     */
    public void forceRecycleAllSegments()
    {
        segmentManager.forceRecycleAll(Collections.emptyList());
    }

    /**
     * Forces a disk flush on the commit log files that need it.  Blocking.
     */
    public void sync(boolean flush) throws IOException
    {
        segmentManager.sync(flush);
    }

    /**
     * Preempts the CLExecutor, telling to to sync immediately
     */
    public void requestExtraSync()
    {
        executor.requestExtraSync();
    }

    /**
     * Add a Mutation to the commit log. If CDC is enabled, this can fail.
     *
     * @param mutation the Mutation to add to the log
     * @throws CDCWriteException
     */
    public CommitLogPosition add(Mutation mutation) throws CDCWriteException
    {
        assert mutation != null;

        mutation.validateSize(MessagingService.current_version, ENTRY_OVERHEAD_SIZE);

        try (DataOutputBuffer dob = DataOutputBuffer.scratchBuffer.get())
        {
            Mutation.serializer.serialize(mutation, dob, MessagingService.current_version);
            int size = dob.getLength();
            int totalSize = size + ENTRY_OVERHEAD_SIZE;
            Allocation alloc = segmentManager.allocate(mutation, totalSize);

            CRC32 checksum = new CRC32();
            final ByteBuffer buffer = alloc.getBuffer();
            try (BufferedDataOutputStreamPlus dos = new DataOutputBufferFixed(buffer))
            {
                // checksummed length
                dos.writeInt(size);
                updateChecksumInt(checksum, size);
                buffer.putInt((int) checksum.getValue());

                // checksummed mutation
                dos.write(dob.getData(), 0, size);
                updateChecksum(checksum, buffer, buffer.position() - size, size);
                buffer.putInt((int) checksum.getValue());
            }
            catch (IOException e)
            {
                throw new FSWriteError(e, alloc.getSegment().getPath());
            }
            finally
            {
                alloc.markWritten();
            }

            executor.finishWriteFor(alloc);
            return alloc.getCommitLogPosition();
        }
        catch (IOException e)
        {
            throw new FSWriteError(e, segmentManager.allocatingFrom().getPath());
        }
    }

    /**
     * Modifies the per-CF dirty cursors of any commit log segments for the column family according to the position
     * given. Discards any commit log segments that are no longer used.
     *
     * @param id         the table that was flushed
     * @param lowerBound the lowest covered replay position of the flush
     * @param lowerBound the highest covered replay position of the flush
     */
    public void discardCompletedSegments(final TableId id, final CommitLogPosition lowerBound, final CommitLogPosition upperBound)
    {
        logger.trace(""discard completed log segments for {}-{}, table {}"", lowerBound, upperBound, id);

        // Go thru the active segment files, which are ordered oldest to newest, marking the
        // flushed CF as clean, until we reach the segment file containing the CommitLogPosition passed
        // in the arguments. Any segments that become unused after they are marked clean will be
        // recycled or discarded.
        for (Iterator<CommitLogSegment> iter = segmentManager.getActiveSegments().iterator(); iter.hasNext();)
        {
            CommitLogSegment segment = iter.next();
            segment.markClean(id, lowerBound, upperBound);

            if (segment.isUnused())
            {
                logger.debug(""Commit log segment {} is unused"", segment);
                segmentManager.archiveAndDiscard(segment);
            }
            else
            {
                if (logger.isTraceEnabled())
                    logger.trace(""Not safe to delete{} commit log segment {}; dirty is {}"",
                            (iter.hasNext() ? """" : "" active""), segment, segment.dirtyString());
            }

            // Don't mark or try to delete any newer segments once we've reached the one containing the
            // position of the flush.
            if (segment.contains(upperBound))
                break;
        }
    }

    @Override
    public String getArchiveCommand()
    {
        return archiver.archiveCommand;
    }

    @Override
    public String getRestoreCommand()
    {
        return archiver.restoreCommand;
    }

    @Override
    public String getRestoreDirectories()
    {
        return archiver.restoreDirectories;
    }

    @Override
    public long getRestorePointInTime()
    {
        return archiver.restorePointInTime;
    }

    @Override
    public String getRestorePrecision()
    {
        return archiver.precision.toString();
    }

    public List<String> getActiveSegmentNames()
    {
        Collection<CommitLogSegment> segments = segmentManager.getActiveSegments();
        List<String> segmentNames = new ArrayList<>(segments.size());
        for (CommitLogSegment seg : segments)
            segmentNames.add(seg.getName());
        return segmentNames;
    }

    public List<String> getArchivingSegmentNames()
    {
        return new ArrayList<>(archiver.archivePending.keySet());
    }

    @Override
    public long getActiveContentSize()
    {
        long size = 0;
        for (CommitLogSegment seg : segmentManager.getActiveSegments())
            size += seg.contentSize();
        return size;
    }

    @Override
    public long getActiveOnDiskSize()
    {
        return segmentManager.onDiskSize();
    }

    @Override
    public Map<String, Double> getActiveSegmentCompressionRatios()
    {
        Map<String, Double> segmentRatios = new TreeMap<>();
        for (CommitLogSegment seg : segmentManager.getActiveSegments())
            segmentRatios.put(seg.getName(), 1.0 * seg.onDiskSize() / seg.contentSize());
        return segmentRatios;
    }

    /**
     * Shuts down the threads used by the commit log, blocking until completion.
     * TODO this should accept a timeout, and throw TimeoutException
     */
    synchronized public void shutdownBlocking() throws InterruptedException
    {
        if (!started)
            return;

        started = false;
        executor.shutdown();
        executor.awaitTermination();
        segmentManager.shutdown();
        segmentManager.awaitTermination();
    }

    /**
     * FOR TESTING PURPOSES
     * @return the number of files recovered
     */
    @VisibleForTesting
    synchronized public int resetUnsafe(boolean deleteSegments) throws IOException
    {
        stopUnsafe(deleteSegments);
        resetConfiguration();
        return restartUnsafe();
    }

    /**
     * FOR TESTING PURPOSES.
     */
    @VisibleForTesting
    synchronized public void resetConfiguration()
    {
        configuration = new Configuration(DatabaseDescriptor.getCommitLogCompression(),
                                          DatabaseDescriptor.getEncryptionContext());
    }

    /**
     * FOR TESTING PURPOSES
     */
    @VisibleForTesting
    synchronized public void stopUnsafe(boolean deleteSegments)
    {
        if (!started)
            return;

        started = false;
        executor.shutdown();
        try
        {
            executor.awaitTermination();
        }
        catch (InterruptedException e)
        {
            throw new RuntimeException(e);
        }
        segmentManager.stopUnsafe(deleteSegments);
        CommitLogSegment.resetReplayLimit();
        if (DatabaseDescriptor.isCDCEnabled() && deleteSegments)
            for (File f : new File(DatabaseDescriptor.getCDCLogLocation()).listFiles())
                FileUtils.deleteWithConfirm(f);
    }

    /**
     * FOR TESTING PURPOSES
     */
    @VisibleForTesting
    synchronized public int restartUnsafe() throws IOException
    {
        started = false;
        return start().recoverSegmentsOnDisk();
    }

    public static long freeDiskSpace()
    {
        return FileUtils.getFreeSpace(new File(DatabaseDescriptor.getCommitLogLocation()));
    }

    @VisibleForTesting
    public static boolean handleCommitError(String message, Throwable t)
    {
        JVMStabilityInspector.inspectCommitLogThrowable(t);
        switch (DatabaseDescriptor.getCommitFailurePolicy())
        {
            // Needed here for unit tests to not fail on default assertion
            case die:
            case stop:
                StorageService.instance.stopTransports();
                //$FALL-THROUGH$
            case stop_commit:
                String errorMsg = String.format(""%s. Commit disk failure policy is %s; terminating thread."", message, DatabaseDescriptor.getCommitFailurePolicy());
                logger.error(addAdditionalInformationIfPossible(errorMsg), t);
                return false;
            case ignore:
                logger.error(addAdditionalInformationIfPossible(message), t);
                return true;
            default:
                throw new AssertionError(DatabaseDescriptor.getCommitFailurePolicy());
        }
    }

    /**
     * Add additional information to the error message if the commit directory does not have enough free space.
     *
     * @param msg the original error message
     * @return the message with additional information if possible
     */
    private static String addAdditionalInformationIfPossible(String msg)
    {
        long unallocatedSpace = freeDiskSpace();
        int segmentSize = DatabaseDescriptor.getCommitLogSegmentSize();

        if (unallocatedSpace < segmentSize)
        {
            return String.format(""%s. %d bytes required for next commitlog segment but only %d bytes available. Check %s to see if not enough free space is the reason for this error."",
                                 msg, segmentSize, unallocatedSpace, DatabaseDescriptor.getCommitLogLocation());
        }
        return msg;
    }

    public static final class Configuration
    {
        /**
         * The compressor class.
         */
        private final ParameterizedClass compressorClass;

        /**
         * The compressor used to compress the segments.
         */
        private final ICompressor compressor;

        /**
         * The encryption context used to encrypt the segments.
         */
        private EncryptionContext encryptionContext;

        public Configuration(ParameterizedClass compressorClass, EncryptionContext encryptionContext)
        {
            this.compressorClass = compressorClass;
            this.compressor = compressorClass != null ? CompressionParams.createCompressor(compressorClass) : null;
            this.encryptionContext = encryptionContext;
        }

        /**
         * Checks if the segments must be compressed.
         * @return <code>true</code> if the segments must be compressed, <code>false</code> otherwise.
         */
        public boolean useCompression()
        {
            return compressor != null;
        }

        /**
         * Checks if the segments must be encrypted.
         * @return <code>true</code> if the segments must be encrypted, <code>false</code> otherwise.
         */
        public boolean useEncryption()
        {
            return encryptionContext.isEnabled();
        }

        /**
         * Returns the compressor used to compress the segments.
         * @return the compressor used to compress the segments
         */
        public ICompressor getCompressor()
        {
            return compressor;
        }

        /**
         * Returns the compressor class.
         * @return the compressor class
         */
        public ParameterizedClass getCompressorClass()
        {
            return compressorClass;
        }

        /**
         * Returns the compressor name.
         * @return the compressor name.
         */
        public String getCompressorName()
        {
            return useCompression() ? compressor.getClass().getSimpleName() : ""none"";
        }

        /**
         * Returns the encryption context used to encrypt the segments.
         * @return the encryption context used to encrypt the segments
         */
        public EncryptionContext getEncryptionContext()
        {
            return encryptionContext;
        }
    }
}


CommitLog.class

public static final CommitLog 

CommitLog.construct

private static CommitLog 

CommitLog 

CommitLog(CommitLogArchiver.construct(), DatabaseDescriptor.getCommitLogSegmentMgrProvider())

@VisibleForTesting
    CommitLog(CommitLogArchiver archiver)
    {
        this(archiver, DatabaseDescriptor.getCommitLogSegmentMgrProvider());
    }

    

@VisibleForTesting
    CommitLog(CommitLogArchiver archiver, Function<CommitLog, AbstractCommitLogSegmentManager> segmentManagerProvider)
    {
        this.configuration = new Configuration(DatabaseDescriptor.getCommitLogCompression(),
                                               DatabaseDescriptor.getEncryptionContext());
        DatabaseDescriptor.createAllDirectories();

        this.archiver = archiver;
        metrics = new CommitLogMetrics();

        switch (DatabaseDescriptor.getCommitLogSync())
        {
            case periodic:
                executor = new PeriodicCommitLogService(this);
                break;
            case batch:
                executor = new BatchCommitLogService(this);
                break;
            case group:
                executor = new GroupCommitLogService(this);
                break;
            default:
                throw new IllegalArgumentException(""Unknown commitlog service type: "" + DatabaseDescriptor.getCommitLogSync());
        }

        segmentManager = segmentManagerProvider.apply(this);

        // register metrics
        metrics.attach(executor, segmentManager);
    }

    

CommitLog, 

synchronized public CommitLog "
org.apache.cassandra.config.DatabaseDescriptor:getEncryptionContext(),transparent_data_encryption_options,(S)org.apache.cassandra.config.DatabaseDescriptor:createAllDirectories(),createAllDirectories,DatabaseDescriptor,../data/xml/cassandra_call_methods/DatabaseDescriptor.xml,"/**
     * Creates all storage-related directories.
     */
public static void createAllDirectories()
    {
        try
        {
            if (conf.data_file_directories.length == 0)
                throw new ConfigurationException(""At least one DataFileDirectory must be specified"", false);

            for (String dataFileDirectory : conf.data_file_directories)
                FileUtils.createDirectory(dataFileDirectory);

            if (conf.local_system_data_file_directory != null)
                FileUtils.createDirectory(conf.local_system_data_file_directory);

            if (conf.commitlog_directory == null)
                throw new ConfigurationException(""commitlog_directory must be specified"", false);
            FileUtils.createDirectory(conf.commitlog_directory);

            if (conf.hints_directory == null)
                throw new ConfigurationException(""hints_directory must be specified"", false);
            FileUtils.createDirectory(conf.hints_directory);

            if (conf.saved_caches_directory == null)
                throw new ConfigurationException(""saved_caches_directory must be specified"", false);
            FileUtils.createDirectory(conf.saved_caches_directory);

            if (conf.cdc_enabled)
            {
                if (conf.cdc_raw_directory == null)
                    throw new ConfigurationException(""cdc_raw_directory must be specified"", false);
                FileUtils.createDirectory(conf.cdc_raw_directory);
            }
        }
        catch (ConfigurationException e)
        {
            throw new IllegalArgumentException(""Bad configuration; unable to start server: ""+e.getMessage());
        }
        catch (FSWriteError e)
        {
            throw new IllegalStateException(e.getCause().getMessage() + ""; unable to start server"");
        }
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getEncryptionContext(),transparent_data_encryption_options,(O)org.apache.cassandra.metrics.CommitLogMetrics:<init>(),<init>,CommitLogMetrics,../data/xml/cassandra_call_methods/CommitLogMetrics.xml,"
public CommitLogMetrics()
    {
        waitingOnSegmentAllocation = Metrics.timer(factory.createMetricName(""WaitingOnSegmentAllocation""));
        waitingOnCommit = Metrics.timer(factory.createMetricName(""WaitingOnCommit""));
        oversizedMutations = Metrics.meter(factory.createMetricName(""OverSizedMutations""));
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getEncryptionContext(),transparent_data_encryption_options,(S)org.apache.cassandra.config.DatabaseDescriptor:getCommitLogSync(),getCommitLogSync,DatabaseDescriptor,../data/xml/cassandra_call_methods/DatabaseDescriptor.xml,"
public static Config.CommitLogSync getCommitLogSync()
    {
        return conf.commitlog_sync;
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getEncryptionContext(),transparent_data_encryption_options,(M)org.apache.cassandra.config.Config$CommitLogSync:ordinal(),ordinal,Config$CommitLogSync,../data/xml/cassandra_call_methods/Config.xml,not found
org.apache.cassandra.config.DatabaseDescriptor:getEncryptionContext(),transparent_data_encryption_options,(O)org.apache.cassandra.db.commitlog.PeriodicCommitLogService:<init>(org.apache.cassandra.db.commitlog.CommitLog),<init>,PeriodicCommitLogService,../data/xml/cassandra_call_methods/PeriodicCommitLogService.xml,"
public PeriodicCommitLogService(final CommitLog commitLog)
    {
        super(commitLog, ""PERIODIC-COMMIT-LOG-SYNCER"", DatabaseDescriptor.getCommitLogSyncPeriod(),
              !(commitLog.configuration.useCompression() || commitLog.configuration.useEncryption()));
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getEncryptionContext(),transparent_data_encryption_options,(O)org.apache.cassandra.db.commitlog.BatchCommitLogService:<init>(org.apache.cassandra.db.commitlog.CommitLog),<init>,BatchCommitLogService,../data/xml/cassandra_call_methods/BatchCommitLogService.xml,"
public BatchCommitLogService(CommitLog commitLog)
    {
        super(commitLog, ""COMMIT-LOG-WRITER"", POLL_TIME_MILLIS);
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getEncryptionContext(),transparent_data_encryption_options,(O)org.apache.cassandra.db.commitlog.GroupCommitLogService:<init>(org.apache.cassandra.db.commitlog.CommitLog),<init>,GroupCommitLogService,../data/xml/cassandra_call_methods/GroupCommitLogService.xml,"
public GroupCommitLogService(CommitLog commitLog)
    {
        super(commitLog, ""GROUP-COMMIT-LOG-WRITER"", (int) DatabaseDescriptor.getCommitLogSyncGroupWindow());
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getEncryptionContext(),transparent_data_encryption_options,"(M)org.apache.cassandra.metrics.CommitLogMetrics:attach(org.apache.cassandra.db.commitlog.AbstractCommitLogService,org.apache.cassandra.db.commitlog.AbstractCommitLogSegmentManager)",attach,CommitLogMetrics,../data/xml/cassandra_call_methods/CommitLogMetrics.xml,"
public void attach(final AbstractCommitLogService service, final AbstractCommitLogSegmentManager segmentManager)
    {
        completedTasks = Metrics.register(factory.createMetricName(""CompletedTasks""), new Gauge<Long>()
        {
            public Long getValue()
            {
                return service.getCompletedTasks();
            }
        });
        pendingTasks = Metrics.register(factory.createMetricName(""PendingTasks""), new Gauge<Long>()
        {
            public Long getValue()
            {
                return service.getPendingTasks();
            }
        });
        totalCommitLogSize = Metrics.register(factory.createMetricName(""TotalCommitLogSize""), new Gauge<Long>()
        {
            public Long getValue()
            {
                return segmentManager.onDiskSize();
            }
        });
    }
}"
org.apache.cassandra.config.DatabaseDescriptor:getEncryptionContext(),transparent_data_encryption_options,(M)org.apache.cassandra.db.commitlog.CommitLogDescriptor:equalsIgnoringCompression(org.apache.cassandra.db.commitlog.CommitLogDescriptor),equalsIgnoringCompression,CommitLogDescriptor,../data/xml/cassandra_call_methods/CommitLogDescriptor.xml,"
public boolean equalsIgnoringCompression(CommitLogDescriptor that)
    {
        return this.version == that.version && this.id == that.id;
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getEncryptionContext(),transparent_data_encryption_options,(S)org.apache.cassandra.schema.CompressionParams:createCompressor(org.apache.cassandra.config.ParameterizedClass),createCompressor,CompressionParams,../data/xml/cassandra_call_methods/CompressionParams.xml,"
private static ICompressor createCompressor(Class<?> compressorClass, Map<String, String> compressionOptions) throws ConfigurationException
    {
        if (compressorClass == null)
        {
            if (!compressionOptions.isEmpty())
                throw new ConfigurationException(""Unknown compression options ("" + compressionOptions.keySet() + "") since no compression class found"");
            return null;
        }

        if (compressionOptions.containsKey(CRC_CHECK_CHANCE))
        {
            if (!hasLoggedCrcCheckChanceWarning)
            {
                logger.warn(CRC_CHECK_CHANCE_WARNING);
                hasLoggedCrcCheckChanceWarning = true;
            }
            compressionOptions.remove(CRC_CHECK_CHANCE);
        }

        try
        {
            Method method = compressorClass.getMethod(""create"", Map.class);
            ICompressor compressor = (ICompressor)method.invoke(null, compressionOptions);
            // Check for unknown options
            for (String provided : compressionOptions.keySet())
                if (!compressor.supportedOptions().contains(provided))
                    throw new ConfigurationException(""Unknown compression options "" + provided);
            return compressor;
        }
        catch (NoSuchMethodException e)
        {
            throw new ConfigurationException(""create method not found"", e);
        }
        catch (SecurityException e)
        {
            throw new ConfigurationException(""Access forbiden"", e);
        }
        catch (IllegalAccessException e)
        {
            throw new ConfigurationException(""Cannot access method create in "" + compressorClass.getName(), e);
        }
        catch (InvocationTargetException e)
        {
            if (e.getTargetException() instanceof ConfigurationException)
                throw (ConfigurationException) e.getTargetException();

            Throwable cause = e.getCause() == null
                            ? e
                            : e.getCause();

            throw new ConfigurationException(format(""%s.create() threw an error: %s %s"",
                                                    compressorClass.getSimpleName(),
                                                    cause.getClass().getName(),
                                                    cause.getMessage()),
                                             e);
        }
        catch (ExceptionInInitializerError e)
        {
            throw new ConfigurationException(""Cannot initialize class "" + compressorClass.getName());
        }
    }

    

public static ICompressor createCompressor(ParameterizedClass compression) throws ConfigurationException
    {
        return createCompressor(parseCompressorClass(compression.class_name), copyOptions(compression.parameters));
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getEncryptionContext(),transparent_data_encryption_options,(S)org.apache.cassandra.config.DatabaseDescriptor:getCommitLogLocation(),getCommitLogLocation,DatabaseDescriptor,../data/xml/cassandra_call_methods/DatabaseDescriptor.xml,"
public static String getCommitLogLocation()
    {
        return conf.commitlog_directory;
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getEncryptionContext(),transparent_data_encryption_options,(M)org.apache.cassandra.db.commitlog.CommitLogDescriptor:fileName(),fileName,CommitLogDescriptor,../data/xml/cassandra_call_methods/CommitLogDescriptor.xml,"
public String fileName()
    {
        return FILENAME_PREFIX + version + SEPARATOR + id + FILENAME_EXTENSION;
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getEncryptionContext(),transparent_data_encryption_options,(M)org.apache.cassandra.io.util.RandomAccessReader:readInt(),readInt,RandomAccessReader,../data/xml/cassandra_call_methods/RandomAccessReader.xml,not found
org.apache.cassandra.config.DatabaseDescriptor:getEncryptionContext(),transparent_data_encryption_options,(M)org.apache.cassandra.io.util.RandomAccessReader:close(),close,RandomAccessReader,../data/xml/cassandra_call_methods/RandomAccessReader.xml,"
@Override
    public void close()
    {
        // close needs to be idempotent.
        if (buffer == null)
            return;

        bufferHolder.release();
        rebufferer.closeReader();
        buffer = null;
        bufferHolder = null;

        //For performance reasons we don't keep a reference to the file
        //channel so we don't close it
    }

    

@Override
        public void close()
        {
            try
            {
                super.close();
            }
            finally
            {
                try
                {
                    rebufferer.close();
                }
                finally
                {
                    getChannel().close();
                }
            }
        }
    }"
org.apache.cassandra.config.DatabaseDescriptor:getEncryptionContext(),transparent_data_encryption_options,(I)org.apache.cassandra.db.commitlog.CommitLogReadHandler:handleUnrecoverableError(org.apache.cassandra.db.commitlog.CommitLogReadHandler$CommitLogReadException),handleUnrecoverableError,CommitLogReadHandler,../data/xml/cassandra_call_methods/CommitLogReadHandler.xml,not found
org.apache.cassandra.config.DatabaseDescriptor:getEncryptionContext(),transparent_data_encryption_options,(I)org.apache.cassandra.db.commitlog.CommitLogReadHandler:shouldSkipSegmentOnError(org.apache.cassandra.db.commitlog.CommitLogReadHandler$CommitLogReadException),shouldSkipSegmentOnError,CommitLogReadHandler,../data/xml/cassandra_call_methods/CommitLogReadHandler.xml,not found
org.apache.cassandra.config.DatabaseDescriptor:getEncryptionContext(),transparent_data_encryption_options,"(O)org.apache.cassandra.db.commitlog.CommitLogSegmentReader:<init>(org.apache.cassandra.db.commitlog.CommitLogReadHandler,org.apache.cassandra.db.commitlog.CommitLogDescriptor,org.apache.cassandra.io.util.RandomAccessReader,boolean)",<init>,CommitLogSegmentReader,../data/xml/cassandra_call_methods/CommitLogSegmentReader.xml,"
protected CommitLogSegmentReader(CommitLogReadHandler handler,
                                     CommitLogDescriptor descriptor,
                                     RandomAccessReader reader,
                                     boolean tolerateTruncation)
    {
        this.handler = handler;
        this.descriptor = descriptor;
        this.reader = reader;
        this.tolerateTruncation = tolerateTruncation;

        end = (int) reader.getFilePointer();
        if (descriptor.getEncryptionContext().isEnabled())
            segmenter = new EncryptedSegmenter(descriptor, reader);
        else if (descriptor.compression != null)
            segmenter = new CompressedSegmenter(descriptor, reader);
        else
            segmenter = new NoOpSegmenter(reader);
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getEncryptionContext(),transparent_data_encryption_options,"(O)org.apache.cassandra.db.commitlog.CommitLogReader$ReadStatusTracker:<init>(int,boolean)",<init>,CommitLogReader$ReadStatusTracker,../data/xml/cassandra_call_methods/CommitLogReader.xml,"
public class CommitLogReader
{
    private static final Logger logger = LoggerFactory.getLogger(CommitLogReader.class);

    private static final int LEGACY_END_OF_SEGMENT_MARKER = 0;

    @VisibleForTesting
    public static final int ALL_MUTATIONS = -1;
    private final CRC32 checksum;
    private final Map<TableId, AtomicInteger> invalidMutations;

    private byte[] buffer;

    public CommitLogReader()
    {
        checksum = new CRC32();
        invalidMutations = new HashMap<>();
        buffer = new byte[4096];
    }

    public Set<Map.Entry<TableId, AtomicInteger>> getInvalidMutations()
    {
        return invalidMutations.entrySet();
    }

    /**
     * Reads all passed in files with no minimum, no start, and no mutation limit.
     */
    public void readAllFiles(CommitLogReadHandler handler, File[] files) throws IOException
    {
        readAllFiles(handler, files, CommitLogPosition.NONE);
    }

    private static boolean shouldSkip(File file) throws IOException, ConfigurationException
    {
        try(RandomAccessReader reader = RandomAccessReader.open(file))
        {
            CommitLogDescriptor.readHeader(reader, DatabaseDescriptor.getEncryptionContext());
            int end = reader.readInt();
            long filecrc = reader.readInt() & 0xffffffffL;
            return end == 0 && filecrc == 0;
        }
    }

    static List<File> filterCommitLogFiles(File[] toFilter)
    {
        List<File> filtered = new ArrayList<>(toFilter.length);
        for (File file: toFilter)
        {
            try
            {
                if (shouldSkip(file))
                {
                    logger.info(""Skipping playback of empty log: {}"", file.getName());
                }
                else
                {
                    filtered.add(file);
                }
            }
            catch (Exception e)
            {
                // let recover deal with it
                filtered.add(file);
            }
        }

        return filtered;
    }

    /**
     * Reads all passed in files with minPosition, no start, and no mutation limit.
     */
    public void readAllFiles(CommitLogReadHandler handler, File[] files, CommitLogPosition minPosition) throws IOException
    {
        List<File> filteredLogs = filterCommitLogFiles(files);
        int i = 0;
        for (File file: filteredLogs)
        {
            i++;
            readCommitLogSegment(handler, file, minPosition, ALL_MUTATIONS, i == filteredLogs.size());
        }
    }

    /**
     * Reads passed in file fully
     */
    public void readCommitLogSegment(CommitLogReadHandler handler, File file, boolean tolerateTruncation) throws IOException
    {
        readCommitLogSegment(handler, file, CommitLogPosition.NONE, ALL_MUTATIONS, tolerateTruncation);
    }

    /**
     * Reads all mutations from passed in file from minPosition
     */
    public void readCommitLogSegment(CommitLogReadHandler handler, File file, CommitLogPosition minPosition, boolean tolerateTruncation) throws IOException
    {
        readCommitLogSegment(handler, file, minPosition, ALL_MUTATIONS, tolerateTruncation);
    }

    /**
     * Reads passed in file fully, up to mutationLimit count
     */
    @VisibleForTesting
    public void readCommitLogSegment(CommitLogReadHandler handler, File file, int mutationLimit, boolean tolerateTruncation) throws IOException
    {
        readCommitLogSegment(handler, file, CommitLogPosition.NONE, mutationLimit, tolerateTruncation);
    }

    /**
     * Reads mutations from file, handing them off to handler
     * @param handler Handler that will take action based on deserialized Mutations
     * @param file CommitLogSegment file to read
     * @param minPosition Optional minimum CommitLogPosition - all segments with id larger or matching w/greater position will be read
     * @param mutationLimit Optional limit on # of mutations to replay. Local ALL_MUTATIONS serves as marker to play all.
     * @param tolerateTruncation Whether or not we should allow truncation of this file or throw if EOF found
     *
     * @throws IOException
     */
    public void readCommitLogSegment(CommitLogReadHandler handler,
                                     File file,
                                     CommitLogPosition minPosition,
                                     int mutationLimit,
                                     boolean tolerateTruncation) throws IOException
    {
        // just transform from the file name (no reading of headers) to determine version
        CommitLogDescriptor desc = CommitLogDescriptor.fromFileName(file.getName());

        try(RandomAccessReader reader = RandomAccessReader.open(file))
        {
            final long segmentIdFromFilename = desc.id;
            try
            {
                // The following call can either throw or legitimately return null. For either case, we need to check
                // desc outside this block and set it to null in the exception case.
                desc = CommitLogDescriptor.readHeader(reader, DatabaseDescriptor.getEncryptionContext());
            }
            catch (Exception e)
            {
                desc = null;
            }
            if (desc == null)
            {
                // don't care about whether or not the handler thinks we can continue. We can't w/out descriptor.
                // whether or not we can continue depends on whether this is the last segment
                handler.handleUnrecoverableError(new CommitLogReadException(
                    String.format(""Could not read commit log descriptor in file %s"", file),
                    CommitLogReadErrorReason.UNRECOVERABLE_DESCRIPTOR_ERROR,
                    tolerateTruncation));
                return;
            }

            if (segmentIdFromFilename != desc.id)
            {
                if (handler.shouldSkipSegmentOnError(new CommitLogReadException(String.format(
                    ""Segment id mismatch (filename %d, descriptor %d) in file %s"", segmentIdFromFilename, desc.id, file),
                                                                                CommitLogReadErrorReason.RECOVERABLE_DESCRIPTOR_ERROR,
                                                                                false)))
                {
                    return;
                }
            }

            if (shouldSkipSegmentId(file, desc, minPosition))
                return;

            CommitLogSegmentReader segmentReader;
            try
            {
                segmentReader = new CommitLogSegmentReader(handler, desc, reader, tolerateTruncation);
            }
            catch(Exception e)
            {
                handler.handleUnrecoverableError(new CommitLogReadException(
                    String.format(""Unable to create segment reader for commit log file: %s"", e),
                    CommitLogReadErrorReason.UNRECOVERABLE_UNKNOWN_ERROR,
                    tolerateTruncation));
                return;
            }

            try
            {
                ReadStatusTracker statusTracker = new ReadStatusTracker(mutationLimit, tolerateTruncation);
                for (CommitLogSegmentReader.SyncSegment syncSegment : segmentReader)
                {
                    // Only tolerate truncation if we allow in both global and segment
                    statusTracker.tolerateErrorsInSection = tolerateTruncation & syncSegment.toleratesErrorsInSection;

                    // Skip segments that are completely behind the desired minPosition
                    if (desc.id == minPosition.segmentId && syncSegment.endPosition < minPosition.position)
                        continue;

                    statusTracker.errorContext = String.format(""Next section at %d in %s"", syncSegment.fileStartPosition, desc.fileName());

                    readSection(handler, syncSegment.input, minPosition, syncSegment.endPosition, statusTracker, desc);
                    if (!statusTracker.shouldContinue())
                        break;
                }
            }
            // Unfortunately AbstractIterator cannot throw a checked exception, so we check to see if a RuntimeException
            // is wrapping an IOException.
            catch (RuntimeException re)
            {
                if (re.getCause() instanceof IOException)
                    throw (IOException) re.getCause();
                throw re;
            }
            logger.info(""Finished reading {}"", file);
        }
    }

    /**
     * Any segment with id >= minPosition.segmentId is a candidate for read.
     */
    private boolean shouldSkipSegmentId(File file, CommitLogDescriptor desc, CommitLogPosition minPosition)
    {
        logger.debug(""Reading {} (CL version {}, messaging version {}, compression {})"",
            file.getPath(),
            desc.version,
            desc.getMessagingVersion(),
            desc.compression);

        if (minPosition.segmentId > desc.id)
        {
            logger.trace(""Skipping read of fully-flushed {}"", file);
            return true;
        }
        return false;
    }

    /**
     * Reads a section of a file containing mutations
     *
     * @param handler Handler that will take action based on deserialized Mutations
     * @param reader FileDataInput / logical buffer containing commitlog mutations
     * @param minPosition CommitLogPosition indicating when we should start actively replaying mutations
     * @param end logical numeric end of the segment being read
     * @param statusTracker ReadStatusTracker with current state of mutation count, error state, etc
     * @param desc Descriptor for CommitLog serialization
     */
    private void readSection(CommitLogReadHandler handler,
                             FileDataInput reader,
                             CommitLogPosition minPosition,
                             int end,
                             ReadStatusTracker statusTracker,
                             CommitLogDescriptor desc) throws IOException
    {
        // seek rather than deserializing mutation-by-mutation to reach the desired minPosition in this SyncSegment
        if (desc.id == minPosition.segmentId && reader.getFilePointer() < minPosition.position)
            reader.seek(minPosition.position);

        while (statusTracker.shouldContinue() && reader.getFilePointer() < end && !reader.isEOF())
        {
            long mutationStart = reader.getFilePointer();
            if (logger.isTraceEnabled())
                logger.trace(""Reading mutation at {}"", mutationStart);

            long claimedCRC32;
            int serializedSize;
            try
            {
                // We rely on reading serialized size == 0 (LEGACY_END_OF_SEGMENT_MARKER) to identify the end
                // of a segment, which happens naturally due to the 0 padding of the empty segment on creation.
                // However, it's possible with 2.1 era commitlogs that the last mutation ended less than 4 bytes
                // from the end of the file, which means that we'll be unable to read an a full int and instead
                // read an EOF here
                if(end - reader.getFilePointer() < 4)
                {
                    logger.trace(""Not enough bytes left for another mutation in this CommitLog section, continuing"");
                    statusTracker.requestTermination();
                    return;
                }

                // any of the reads may hit EOF
                serializedSize = reader.readInt();
                if (serializedSize == LEGACY_END_OF_SEGMENT_MARKER)
                {
                    logger.trace(""Encountered end of segment marker at {}"", reader.getFilePointer());
                    statusTracker.requestTermination();
                    return;
                }

                // Mutation must be at LEAST 10 bytes:
                //    3 for a non-empty Keyspace
                //    3 for a Key (including the 2-byte length from writeUTF/writeWithShortLength)
                //    4 bytes for column count.
                // This prevents CRC by being fooled by special-case garbage in the file; see CASSANDRA-2128
                if (serializedSize < 10)
                {
                    if (handler.shouldSkipSegmentOnError(new CommitLogReadException(
                                                    String.format(""Invalid mutation size %d at %d in %s"", serializedSize, mutationStart, statusTracker.errorContext),
                                                    CommitLogReadErrorReason.MUTATION_ERROR,
                                                    statusTracker.tolerateErrorsInSection)))
                    {
                        statusTracker.requestTermination();
                    }
                    return;
                }

                long claimedSizeChecksum = CommitLogFormat.calculateClaimedChecksum(reader, desc.version);
                checksum.reset();
                CommitLogFormat.updateChecksum(checksum, serializedSize, desc.version);

                if (checksum.getValue() != claimedSizeChecksum)
                {
                    if (handler.shouldSkipSegmentOnError(new CommitLogReadException(
                                                    String.format(""Mutation size checksum failure at %d in %s"", mutationStart, statusTracker.errorContext),
                                                    CommitLogReadErrorReason.MUTATION_ERROR,
                                                    statusTracker.tolerateErrorsInSection)))
                    {
                        statusTracker.requestTermination();
                    }
                    return;
                }

                if (serializedSize > buffer.length)
                    buffer = new byte[(int) (1.2 * serializedSize)];
                reader.readFully(buffer, 0, serializedSize);

                claimedCRC32 = CommitLogFormat.calculateClaimedCRC32(reader, desc.version);
            }
            catch (EOFException eof)
            {
                if (handler.shouldSkipSegmentOnError(new CommitLogReadException(
                                                String.format(""Unexpected end of segment at %d in %s"", mutationStart, statusTracker.errorContext),
                                                CommitLogReadErrorReason.EOF,
                                                statusTracker.tolerateErrorsInSection)))
                {
                    statusTracker.requestTermination();
                }
                return;
            }

            checksum.update(buffer, 0, serializedSize);
            if (claimedCRC32 != checksum.getValue())
            {
                if (handler.shouldSkipSegmentOnError(new CommitLogReadException(
                                                String.format(""Mutation checksum failure at %d in %s"", mutationStart, statusTracker.errorContext),
                                                CommitLogReadErrorReason.MUTATION_ERROR,
                                                statusTracker.tolerateErrorsInSection)))
                {
                    statusTracker.requestTermination();
                }
                continue;
            }

            long mutationPosition = reader.getFilePointer();
            readMutation(handler, buffer, serializedSize, minPosition, (int)mutationPosition, desc);

            // Only count this as a processed mutation if it is after our min as we suppress reading of mutations that
            // are before this mark.
            if (mutationPosition >= minPosition.position)
                statusTracker.addProcessedMutation();
        }
    }

    /**
     * Deserializes and passes a Mutation to the ICommitLogReadHandler requested
     *
     * @param handler Handler that will take action based on deserialized Mutations
     * @param inputBuffer raw byte array w/Mutation data
     * @param size deserialized size of mutation
     * @param minPosition We need to suppress replay of mutations that are before the required minPosition
     * @param entryLocation filePointer offset of end of mutation within CommitLogSegment
     * @param desc CommitLogDescriptor being worked on
     */
    @VisibleForTesting
    protected void readMutation(CommitLogReadHandler handler,
                                byte[] inputBuffer,
                                int size,
                                CommitLogPosition minPosition,
                                final int entryLocation,
                                final CommitLogDescriptor desc) throws IOException
    {
        // For now, we need to go through the motions of deserializing the mutation to determine its size and move
        // the file pointer forward accordingly, even if we're behind the requested minPosition within this SyncSegment.
        boolean shouldReplay = entryLocation > minPosition.position;

        final Mutation mutation;
        try (RebufferingInputStream bufIn = new DataInputBuffer(inputBuffer, 0, size))
        {
            mutation = Mutation.serializer.deserialize(bufIn,
                                                       desc.getMessagingVersion(),
                                                       DeserializationHelper.Flag.LOCAL);
            // doublecheck that what we read is still] valid for the current schema
            for (PartitionUpdate upd : mutation.getPartitionUpdates())
                upd.validate();
        }
        catch (UnknownTableException ex)
        {
            if (ex.id == null)
                return;
            AtomicInteger i = invalidMutations.get(ex.id);
            if (i == null)
            {
                i = new AtomicInteger(1);
                invalidMutations.put(ex.id, i);
            }
            else
                i.incrementAndGet();
            return;
        }
        catch (Throwable t)
        {
            JVMStabilityInspector.inspectThrowable(t);
            Path p = Files.createTempFile(""mutation"", ""dat"");

            try (DataOutputStream out = new DataOutputStream(Files.newOutputStream(p)))
            {
                out.write(inputBuffer, 0, size);
            }

            // Checksum passed so this error can't be permissible.
            handler.handleUnrecoverableError(new CommitLogReadException(
                String.format(
                    ""Unexpected error deserializing mutation; saved to %s.  "" +
                    ""This may be caused by replaying a mutation against a table with the same name but incompatible schema.  "" +
                    ""Exception follows: %s"", p.toString(), t),
                CommitLogReadErrorReason.MUTATION_ERROR,
                false));
            return;
        }

        if (logger.isTraceEnabled())
            logger.trace(""Read mutation for {}.{}: {}"", mutation.getKeyspaceName(), mutation.key(),
                         ""{"" + StringUtils.join(mutation.getPartitionUpdates().iterator(), "", "") + ""}"");

        if (shouldReplay)
            handler.handleMutation(mutation, size, entryLocation, desc);
    }

    /**
     * Helper methods to deal with changing formats of internals of the CommitLog without polluting deserialization code.
     */
    private static class CommitLogFormat
    {
        public static long calculateClaimedChecksum(FileDataInput input, int commitLogVersion) throws IOException
        {
            return input.readInt() & 0xffffffffL;
        }

        public static void updateChecksum(CRC32 checksum, int serializedSize, int commitLogVersion)
        {
            updateChecksumInt(checksum, serializedSize);
        }

        public static long calculateClaimedCRC32(FileDataInput input, int commitLogVersion) throws IOException
        {
            return input.readInt() & 0xffffffffL;
        }
    }

    private static class ReadStatusTracker
    {
        private int mutationsLeft;
        public String errorContext = """";
        public boolean tolerateErrorsInSection;
        private boolean error;

        public ReadStatusTracker(int mutationLimit, boolean tolerateErrorsInSection)
        {
            this.mutationsLeft = mutationLimit;
            this.tolerateErrorsInSection = tolerateErrorsInSection;
        }

        public void addProcessedMutation()
        {
            if (mutationsLeft == ALL_MUTATIONS)
                return;
            --mutationsLeft;
        }

        public boolean shouldContinue()
        {
            return !error && (mutationsLeft != 0 || mutationsLeft == ALL_MUTATIONS);
        }

        public void requestTermination()
        {
            error = true;
        }
    }
}


CommitLogReader.class

public CommitLogReader()
    {
        checksum = new CRC32();
        invalidMutations = new HashMap<>();
        buffer = new byte[4096];
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getEncryptionContext(),transparent_data_encryption_options,(M)org.apache.cassandra.db.commitlog.CommitLogSegmentReader:iterator(),iterator,CommitLogSegmentReader,../data/xml/cassandra_call_methods/CommitLogSegmentReader.xml,"
public Iterator<SyncSegment> iterator()
    {
        return new SegmentIterator();
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getEncryptionContext(),transparent_data_encryption_options,"(O)org.apache.cassandra.db.commitlog.CommitLogReader:readSection(org.apache.cassandra.db.commitlog.CommitLogReadHandler,org.apache.cassandra.io.util.FileDataInput,org.apache.cassandra.db.commitlog.CommitLogPosition,int,org.apache.cassandra.db.commitlog.CommitLogReader$ReadStatusTracker,org.apache.cassandra.db.commitlog.CommitLogDescriptor)",readSection,CommitLogReader,../data/xml/cassandra_call_methods/CommitLogReader.xml,"/**
     * Reads a section of a file containing mutations
     *
     * @param handler Handler that will take action based on deserialized Mutations
     * @param reader FileDataInput / logical buffer containing commitlog mutations
     * @param minPosition CommitLogPosition indicating when we should start actively replaying mutations
     * @param end logical numeric end of the segment being read
     * @param statusTracker ReadStatusTracker with current state of mutation count, error state, etc
     * @param desc Descriptor for CommitLog serialization
     */
private void readSection(CommitLogReadHandler handler,
                             FileDataInput reader,
                             CommitLogPosition minPosition,
                             int end,
                             ReadStatusTracker statusTracker,
                             CommitLogDescriptor desc) throws IOException
    {
        // seek rather than deserializing mutation-by-mutation to reach the desired minPosition in this SyncSegment
        if (desc.id == minPosition.segmentId && reader.getFilePointer() < minPosition.position)
            reader.seek(minPosition.position);

        while (statusTracker.shouldContinue() && reader.getFilePointer() < end && !reader.isEOF())
        {
            long mutationStart = reader.getFilePointer();
            if (logger.isTraceEnabled())
                logger.trace(""Reading mutation at {}"", mutationStart);

            long claimedCRC32;
            int serializedSize;
            try
            {
                // We rely on reading serialized size == 0 (LEGACY_END_OF_SEGMENT_MARKER) to identify the end
                // of a segment, which happens naturally due to the 0 padding of the empty segment on creation.
                // However, it's possible with 2.1 era commitlogs that the last mutation ended less than 4 bytes
                // from the end of the file, which means that we'll be unable to read an a full int and instead
                // read an EOF here
                if(end - reader.getFilePointer() < 4)
                {
                    logger.trace(""Not enough bytes left for another mutation in this CommitLog section, continuing"");
                    statusTracker.requestTermination();
                    return;
                }

                // any of the reads may hit EOF
                serializedSize = reader.readInt();
                if (serializedSize == LEGACY_END_OF_SEGMENT_MARKER)
                {
                    logger.trace(""Encountered end of segment marker at {}"", reader.getFilePointer());
                    statusTracker.requestTermination();
                    return;
                }

                // Mutation must be at LEAST 10 bytes:
                //    3 for a non-empty Keyspace
                //    3 for a Key (including the 2-byte length from writeUTF/writeWithShortLength)
                //    4 bytes for column count.
                // This prevents CRC by being fooled by special-case garbage in the file; see CASSANDRA-2128
                if (serializedSize < 10)
                {
                    if (handler.shouldSkipSegmentOnError(new CommitLogReadException(
                                                    String.format(""Invalid mutation size %d at %d in %s"", serializedSize, mutationStart, statusTracker.errorContext),
                                                    CommitLogReadErrorReason.MUTATION_ERROR,
                                                    statusTracker.tolerateErrorsInSection)))
                    {
                        statusTracker.requestTermination();
                    }
                    return;
                }

                long claimedSizeChecksum = CommitLogFormat.calculateClaimedChecksum(reader, desc.version);
                checksum.reset();
                CommitLogFormat.updateChecksum(checksum, serializedSize, desc.version);

                if (checksum.getValue() != claimedSizeChecksum)
                {
                    if (handler.shouldSkipSegmentOnError(new CommitLogReadException(
                                                    String.format(""Mutation size checksum failure at %d in %s"", mutationStart, statusTracker.errorContext),
                                                    CommitLogReadErrorReason.MUTATION_ERROR,
                                                    statusTracker.tolerateErrorsInSection)))
                    {
                        statusTracker.requestTermination();
                    }
                    return;
                }

                if (serializedSize > buffer.length)
                    buffer = new byte[(int) (1.2 * serializedSize)];
                reader.readFully(buffer, 0, serializedSize);

                claimedCRC32 = CommitLogFormat.calculateClaimedCRC32(reader, desc.version);
            }
            catch (EOFException eof)
            {
                if (handler.shouldSkipSegmentOnError(new CommitLogReadException(
                                                String.format(""Unexpected end of segment at %d in %s"", mutationStart, statusTracker.errorContext),
                                                CommitLogReadErrorReason.EOF,
                                                statusTracker.tolerateErrorsInSection)))
                {
                    statusTracker.requestTermination();
                }
                return;
            }

            checksum.update(buffer, 0, serializedSize);
            if (claimedCRC32 != checksum.getValue())
            {
                if (handler.shouldSkipSegmentOnError(new CommitLogReadException(
                                                String.format(""Mutation checksum failure at %d in %s"", mutationStart, statusTracker.errorContext),
                                                CommitLogReadErrorReason.MUTATION_ERROR,
                                                statusTracker.tolerateErrorsInSection)))
                {
                    statusTracker.requestTermination();
                }
                continue;
            }

            long mutationPosition = reader.getFilePointer();
            readMutation(handler, buffer, serializedSize, minPosition, (int)mutationPosition, desc);

            // Only count this as a processed mutation if it is after our min as we suppress reading of mutations that
            // are before this mark.
            if (mutationPosition >= minPosition.position)
                statusTracker.addProcessedMutation();
        }
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getEncryptionContext(),transparent_data_encryption_options,(M)org.apache.cassandra.db.commitlog.CommitLogReader$ReadStatusTracker:shouldContinue(),shouldContinue,CommitLogReader$ReadStatusTracker,../data/xml/cassandra_call_methods/CommitLogReader.xml,"
public boolean shouldContinue()
        {
            return !error && (mutationsLeft != 0 || mutationsLeft == ALL_MUTATIONS);
        }

        "
org.apache.cassandra.config.DatabaseDescriptor:getEncryptionContext(),transparent_data_encryption_options,(M)org.apache.cassandra.db.commitlog.CommitLogSegmentManagerCDC:addCDCSize(long),addCDCSize,CommitLogSegmentManagerCDC,../data/xml/cassandra_call_methods/CommitLogSegmentManagerCDC.xml,"/**
     * For use after replay when replayer hard-links / adds tracking of replayed segments
     */
public void addCDCSize(long size)
    {
        cdcSizeTracker.addSize(size);
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getEncryptionContext(),transparent_data_encryption_options,(S)org.apache.cassandra.config.DatabaseDescriptor:getCDCLogLocation(),getCDCLogLocation,DatabaseDescriptor,../data/xml/cassandra_call_methods/DatabaseDescriptor.xml,"
public static String getCDCLogLocation()
    {
        return conf.cdc_raw_directory;
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getEncryptionContext(),transparent_data_encryption_options,"(S)org.apache.cassandra.db.commitlog.CommitLogSegment:writeCDCIndexFile(org.apache.cassandra.db.commitlog.CommitLogDescriptor,int,boolean)",writeCDCIndexFile,CommitLogSegment,../data/xml/cassandra_call_methods/CommitLogSegment.xml,"/**
     * We persist the offset of the last data synced to disk so clients can parse only durable data if they choose. Data
     * in shared / memory-mapped buffers reflects un-synced data so we need an external sentinel for clients to read to
     * determine actual durable data persisted.
     */
public static void writeCDCIndexFile(CommitLogDescriptor desc, int offset, boolean complete)
    {
        try(FileWriter writer = new FileWriter(new File(DatabaseDescriptor.getCDCLogLocation(), desc.cdcIndexFileName())))
        {
            writer.write(String.valueOf(offset));
            if (complete)
                writer.write(""\nCOMPLETED"");
            writer.flush();
        }
        catch (IOException e)
        {
            if (!CommitLog.instance.handleCommitError(""Failed to sync CDC Index: "" + desc.cdcIndexFileName(), e))
                throw new RuntimeException(e);
        }
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getEncryptionContext(),transparent_data_encryption_options,(M)org.apache.cassandra.security.EncryptionContext:getDecryptor(),getDecryptor,EncryptionContext,../data/xml/cassandra_call_methods/EncryptionContext.xml,"
public Cipher getDecryptor() throws IOException
    {
        if (iv == null || iv.length == 0)
            throw new IllegalStateException(""no initialization vector (IV) found in this context"");
        return cipherFactory.getDecryptor(tdeOptions.cipher, tdeOptions.key_alias, iv);
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getEncryptionContext(),transparent_data_encryption_options,(M)org.apache.cassandra.security.EncryptionContext:getEncryptor(),getEncryptor,EncryptionContext,../data/xml/cassandra_call_methods/EncryptionContext.xml,"
public Cipher getEncryptor() throws IOException
    {
        return cipherFactory.getEncryptor(tdeOptions.cipher, tdeOptions.key_alias);
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getEncryptionContext(),transparent_data_encryption_options,(M)org.apache.cassandra.security.EncryptionContext:toHeaderParameters(),toHeaderParameters,EncryptionContext,../data/xml/cassandra_call_methods/EncryptionContext.xml,"
public Map<String, String> toHeaderParameters()
    {
        Map<String, String> map = new HashMap<>(3);
        // add compression options, someday ...
        if (tdeOptions.enabled)
        {
            map.put(ENCRYPTION_CIPHER, tdeOptions.cipher);
            map.put(ENCRYPTION_KEY_ALIAS, tdeOptions.key_alias);

            if (iv != null && iv.length > 0)
                map.put(ENCRYPTION_IV, Hex.bytesToHex(iv));
        }
        return map;
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getEncryptionContext(),transparent_data_encryption_options,(S)org.apache.cassandra.utils.Hex:bytesToHex(byte[]),bytesToHex,Hex,../data/xml/cassandra_call_methods/Hex.xml,"
public static String bytesToHex(byte... bytes)
    {
        return bytesToHex(bytes, 0, bytes.length);
    }

    

public static String bytesToHex(byte bytes[], int offset, int length)
    {
        char[] c = new char[length * 2];
        for (int i = 0; i < length; i++)
        {
            int bint = bytes[i + offset];
            c[i * 2] = byteToChar[(bint & 0xf0) >> 4];
            c[1 + i * 2] = byteToChar[bint & 0x0f];
        }

        return wrapCharArray(c);
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getEncryptionContext(),transparent_data_encryption_options,(M)org.apache.cassandra.security.EncryptionContext:getCompressor(),getCompressor,EncryptionContext,../data/xml/cassandra_call_methods/EncryptionContext.xml,"
public ICompressor getCompressor()
    {
        return compressor;
    }

    "
