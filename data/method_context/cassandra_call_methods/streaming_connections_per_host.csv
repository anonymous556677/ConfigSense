function,option,Method,Method_short,class_name,xml_path,Method_body
org.apache.cassandra.config.DatabaseDescriptor:getStreamingConnectionsPerHost(),streaming_connections_per_host,(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),getEndpointSnitch,DatabaseDescriptor,../data/xml/cassandra_call_methods/DatabaseDescriptor.xml,"
public static IEndpointSnitch getEndpointSnitch()
    {
        return snitch;
    }
    "
org.apache.cassandra.config.DatabaseDescriptor:getStreamingConnectionsPerHost(),streaming_connections_per_host,(S)org.apache.cassandra.config.DatabaseDescriptor:getStreamingConnectionsPerHost(),getStreamingConnectionsPerHost,DatabaseDescriptor,../data/xml/cassandra_call_methods/DatabaseDescriptor.xml,"
public static int getStreamingConnectionsPerHost()
    {
        return conf.streaming_connections_per_host;
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getStreamingConnectionsPerHost(),streaming_connections_per_host,(M)org.apache.cassandra.schema.Schema:getNonLocalStrategyKeyspaces(),getNonLocalStrategyKeyspaces,Schema,../data/xml/cassandra_call_methods/Schema.xml,"/**
     * @return a collection of keyspaces that do not use LocalStrategy for replication
     */
public List<String> getNonLocalStrategyKeyspaces()
    {
        return keyspaces.stream()
                        .filter(keyspace -> keyspace.params.replication.klass != LocalStrategy.class)
                        .map(keyspace -> keyspace.name)
                        .collect(Collectors.toList());
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getStreamingConnectionsPerHost(),streaming_connections_per_host,(M)org.apache.cassandra.db.Keyspace:getReplicationStrategy(),getReplicationStrategy,Keyspace,../data/xml/cassandra_call_methods/Keyspace.xml,"
public AbstractReplicationStrategy getReplicationStrategy()
    {
        return replicationStrategy;
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getStreamingConnectionsPerHost(),streaming_connections_per_host,(M)org.apache.cassandra.dht.RangeStreamer:fetchAsync(),fetchAsync,RangeStreamer,../data/xml/cassandra_call_methods/RangeStreamer.xml,"
public StreamResultFuture fetchAsync()
    {
        toFetch.forEach((keyspace, sources) -> {
            logger.debug(""Keyspace {} Sources {}"", keyspace, sources);
            sources.asMap().forEach((source, fetchReplicas) -> {

                // filter out already streamed ranges
                SystemKeyspace.AvailableRanges available = stateStore.getAvailableRanges(keyspace, metadata.partitioner);

                Predicate<FetchReplica> isAvailable = fetch -> {
                    boolean isInFull = available.full.contains(fetch.local.range());
                    boolean isInTrans = available.trans.contains(fetch.local.range());

                    if (!isInFull && !isInTrans)
                        //Range is unavailable
                        return false;

                    if (fetch.local.isFull())
                        //For full, pick only replicas with matching transientness
                        return isInFull == fetch.remote.isFull();

                    // Any transient or full will do
                    return true;
                };

                List<FetchReplica> remaining = fetchReplicas.stream().filter(not(isAvailable)).collect(Collectors.toList());

                if (remaining.size() < available.full.size() + available.trans.size())
                {
                    List<FetchReplica> skipped = fetchReplicas.stream().filter(isAvailable).collect(Collectors.toList());
                    logger.info(""Some ranges of {} are already available. Skipping streaming those ranges. Skipping {}. Fully available {} Transiently available {}"",
                                fetchReplicas, skipped, available.full, available.trans);
                }

                if (logger.isTraceEnabled())
                    logger.trace(""{}ing from {} ranges {}"", description, source, StringUtils.join(remaining, "", ""));

                InetAddressAndPort self = FBUtilities.getBroadcastAddressAndPort();
                RangesAtEndpoint full = remaining.stream()
                        .filter(pair -> pair.remote.isFull())
                        .map(pair -> pair.local)
                        .collect(RangesAtEndpoint.collector(self));
                RangesAtEndpoint transientReplicas = remaining.stream()
                        .filter(pair -> pair.remote.isTransient())
                        .map(pair -> pair.local)
                        .collect(RangesAtEndpoint.collector(self));

                logger.debug(""Source and our replicas {}"", fetchReplicas);
                logger.debug(""Source {} Keyspace {}  streaming full {} transient {}"", source, keyspace, full, transientReplicas);

                /* Send messages to respective folks to stream data over to me */
                streamPlan.requestRanges(source, keyspace, full, transientReplicas);
            });
        });

        return streamPlan.execute();
    }
}"
org.apache.cassandra.config.DatabaseDescriptor:getStreamingConnectionsPerHost(),streaming_connections_per_host,(O)org.apache.cassandra.dht.BootStrapper$1:<init>(org.apache.cassandra.dht.BootStrapper),<init>,BootStrapper$1,../data/xml/cassandra_call_methods/BootStrapper.xml,"
public class BootStrapper extends ProgressEventNotifierSupport
{
    private static final Logger logger = LoggerFactory.getLogger(BootStrapper.class);

    /* endpoint that needs to be bootstrapped */
    protected final InetAddressAndPort address;
    /* token of the node being bootstrapped. */
    protected final Collection<Token> tokens;
    protected final TokenMetadata tokenMetadata;

    public BootStrapper(InetAddressAndPort address, Collection<Token> tokens, TokenMetadata tmd)
    {
        assert address != null;
        assert tokens != null && !tokens.isEmpty();

        this.address = address;
        this.tokens = tokens;
        this.tokenMetadata = tmd;
    }

    public ListenableFuture<StreamState> bootstrap(StreamStateStore stateStore, boolean useStrictConsistency)
    {
        logger.trace(""Beginning bootstrap process"");

        RangeStreamer streamer = new RangeStreamer(tokenMetadata,
                                                   tokens,
                                                   address,
                                                   StreamOperation.BOOTSTRAP,
                                                   useStrictConsistency,
                                                   DatabaseDescriptor.getEndpointSnitch(),
                                                   stateStore,
                                                   true,
                                                   DatabaseDescriptor.getStreamingConnectionsPerHost());
        final List<String> nonLocalStrategyKeyspaces = Schema.instance.getNonLocalStrategyKeyspaces();
        if (nonLocalStrategyKeyspaces.isEmpty())
            logger.debug(""Schema does not contain any non-local keyspaces to stream on bootstrap"");
        for (String keyspaceName : nonLocalStrategyKeyspaces)
        {
            AbstractReplicationStrategy strategy = Keyspace.open(keyspaceName).getReplicationStrategy();
            streamer.addRanges(keyspaceName, strategy.getPendingAddressRanges(tokenMetadata, tokens, address));
        }

        StreamResultFuture bootstrapStreamResult = streamer.fetchAsync();
        bootstrapStreamResult.addEventListener(new StreamEventHandler()
        {
            private final AtomicInteger receivedFiles = new AtomicInteger();
            private final AtomicInteger totalFilesToReceive = new AtomicInteger();

            @Override
            public void handleStreamEvent(StreamEvent event)
            {
                switch (event.eventType)
                {
                    case STREAM_PREPARED:
                        StreamEvent.SessionPreparedEvent prepared = (StreamEvent.SessionPreparedEvent) event;
                        int currentTotal = totalFilesToReceive.addAndGet((int) prepared.session.getTotalFilesToReceive());
                        ProgressEvent prepareProgress = new ProgressEvent(ProgressEventType.PROGRESS, receivedFiles.get(), currentTotal, ""prepare with "" + prepared.session.peer + "" complete"");
                        fireProgressEvent(""bootstrap"", prepareProgress);
                        break;

                    case FILE_PROGRESS:
                        StreamEvent.ProgressEvent progress = (StreamEvent.ProgressEvent) event;
                        if (progress.progress.isCompleted())
                        {
                            int received = receivedFiles.incrementAndGet();
                            ProgressEvent currentProgress = new ProgressEvent(ProgressEventType.PROGRESS, received, totalFilesToReceive.get(), ""received file "" + progress.progress.fileName);
                            fireProgressEvent(""bootstrap"", currentProgress);
                        }
                        break;

                    case STREAM_COMPLETE:
                        StreamEvent.SessionCompleteEvent completeEvent = (StreamEvent.SessionCompleteEvent) event;
                        ProgressEvent completeProgress = new ProgressEvent(ProgressEventType.PROGRESS, receivedFiles.get(), totalFilesToReceive.get(), ""session with "" + completeEvent.peer + "" complete"");
                        fireProgressEvent(""bootstrap"", completeProgress);
                        break;
                }
            }

            @Override
            public void onSuccess(StreamState streamState)
            {
                ProgressEventType type;
                String message;

                if (streamState.hasFailedSession())
                {
                    type = ProgressEventType.ERROR;
                    message = ""Some bootstrap stream failed"";
                }
                else
                {
                    type = ProgressEventType.SUCCESS;
                    message = ""Bootstrap streaming success"";
                }
                ProgressEvent currentProgress = new ProgressEvent(type, receivedFiles.get(), totalFilesToReceive.get(), message);
                fireProgressEvent(""bootstrap"", currentProgress);
            }

            @Override
            public void onFailure(Throwable throwable)
            {
                ProgressEvent currentProgress = new ProgressEvent(ProgressEventType.ERROR, receivedFiles.get(), totalFilesToReceive.get(), throwable.getMessage());
                fireProgressEvent(""bootstrap"", currentProgress);
            }
        });
        return bootstrapStreamResult;
    }

    /**
     * if initialtoken was specified, use that (split on comma).
     * otherwise, if allocationKeyspace is specified use the token allocation algorithm to generate suitable tokens
     * else choose num_tokens tokens at random
     */
    public static Collection<Token> getBootstrapTokens(final TokenMetadata metadata, InetAddressAndPort address, long schemaWaitDelay) throws ConfigurationException
    {
        String allocationKeyspace = DatabaseDescriptor.getAllocateTokensForKeyspace();
        Integer allocationLocalRf = DatabaseDescriptor.getAllocateTokensForLocalRf();
        Collection<String> initialTokens = DatabaseDescriptor.getInitialTokens();
        if (initialTokens.size() > 0 && allocationKeyspace != null)
            logger.warn(""manually specified tokens override automatic allocation"");

        // if user specified tokens, use those
        if (initialTokens.size() > 0)
        {
            Collection<Token> tokens = getSpecifiedTokens(metadata, initialTokens);
            BootstrapDiagnostics.useSpecifiedTokens(address, allocationKeyspace, tokens, DatabaseDescriptor.getNumTokens());
            return tokens;
        }

        int numTokens = DatabaseDescriptor.getNumTokens();
        if (numTokens < 1)
            throw new ConfigurationException(""num_tokens must be >= 1"");

        if (allocationKeyspace != null)
            return allocateTokens(metadata, address, allocationKeyspace, numTokens, schemaWaitDelay);

        if (allocationLocalRf != null)
            return allocateTokens(metadata, address, allocationLocalRf, numTokens, schemaWaitDelay);

        if (numTokens == 1)
            logger.warn(""Picking random token for a single vnode.  You should probably add more vnodes and/or use the automatic token allocation mechanism."");

        Collection<Token> tokens = getRandomTokens(metadata, numTokens);
        BootstrapDiagnostics.useRandomTokens(address, metadata, numTokens, tokens);
        return tokens;
    }

    private static Collection<Token> getSpecifiedTokens(final TokenMetadata metadata,
                                                        Collection<String> initialTokens)
    {
        logger.info(""tokens manually specified as {}"",  initialTokens);
        List<Token> tokens = new ArrayList<>(initialTokens.size());
        for (String tokenString : initialTokens)
        {
            Token token = metadata.partitioner.getTokenFactory().fromString(tokenString);
            if (metadata.getEndpoint(token) != null)
                throw new ConfigurationException(""Bootstrapping to existing token "" + tokenString + "" is not allowed (decommission/removenode the old node first)."");
            tokens.add(token);
        }
        return tokens;
    }

    static Collection<Token> allocateTokens(final TokenMetadata metadata,
                                            InetAddressAndPort address,
                                            String allocationKeyspace,
                                            int numTokens,
                                            long schemaWaitDelay)
    {
        StorageService.instance.waitForSchema(schemaWaitDelay);
        if (!FBUtilities.getBroadcastAddressAndPort().equals(InetAddressAndPort.getLoopbackAddress()))
            Gossiper.waitToSettle();

        Keyspace ks = Keyspace.open(allocationKeyspace);
        if (ks == null)
            throw new ConfigurationException(""Problem opening token allocation keyspace "" + allocationKeyspace);
        AbstractReplicationStrategy rs = ks.getReplicationStrategy();

        Collection<Token> tokens = TokenAllocation.allocateTokens(metadata, rs, address, numTokens);
        BootstrapDiagnostics.tokensAllocated(address, metadata, allocationKeyspace, numTokens, tokens);
        return tokens;
    }


    static Collection<Token> allocateTokens(final TokenMetadata metadata,
                                            InetAddressAndPort address,
                                            int rf,
                                            int numTokens,
                                            long schemaWaitDelay)
    {
        StorageService.instance.waitForSchema(schemaWaitDelay);
        if (!FBUtilities.getBroadcastAddressAndPort().equals(InetAddressAndPort.getLoopbackAddress()))
            Gossiper.waitToSettle();

        Collection<Token> tokens = TokenAllocation.allocateTokens(metadata, rf, address, numTokens);
        BootstrapDiagnostics.tokensAllocated(address, metadata, rf, numTokens, tokens);
        return tokens;
    }

    public static Collection<Token> getRandomTokens(TokenMetadata metadata, int numTokens)
    {
        Set<Token> tokens = new HashSet<>(numTokens);
        while (tokens.size() < numTokens)
        {
            Token token = metadata.partitioner.getRandomToken();
            if (metadata.getEndpoint(token) == null)
                tokens.add(token);
        }

        logger.info(""Generated random tokens. tokens are {}"", tokens);
        return tokens;
    }
}


BootStrapper.class

public BootStrapper(InetAddressAndPort address, Collection<Token> tokens, TokenMetadata tmd)
    {
        assert address != null;
        assert tokens != null && !tokens.isEmpty();

        this.address = address;
        this.tokens = tokens;
        this.tokenMetadata = tmd;
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getStreamingConnectionsPerHost(),streaming_connections_per_host,(M)org.apache.cassandra.streaming.StreamResultFuture:addEventListener(org.apache.cassandra.streaming.StreamEventHandler),addEventListener,StreamResultFuture,../data/xml/cassandra_call_methods/StreamResultFuture.xml,"
@SuppressWarnings(""UnstableApiUsage"")
    public void addEventListener(StreamEventHandler listener)
    {
        Futures.addCallback(this, listener, MoreExecutors.directExecutor());
        eventListeners.add(listener);
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getStreamingConnectionsPerHost(),streaming_connections_per_host,(S)org.apache.cassandra.utils.FBUtilities:getBroadcastAddressAndPort(),getBroadcastAddressAndPort,FBUtilities,../data/xml/cassandra_call_methods/FBUtilities.xml,"/**
     * Get the broadcast address and port for intra-cluster storage traffic. This the address to advertise that uniquely
     * identifies the node and is reachable from everywhere. This is the one you want unless you are trying to connect
     * to the local address specifically.
     */
public static InetAddressAndPort getBroadcastAddressAndPort()
    {
        if (broadcastInetAddressAndPort == null)
        {
            if(DatabaseDescriptor.getRawConfig() == null)
            {
                broadcastInetAddressAndPort = InetAddressAndPort.getByAddress(getJustBroadcastAddress());
            }
            else
            {
                broadcastInetAddressAndPort = InetAddressAndPort.getByAddressOverrideDefaults(getJustBroadcastAddress(),
                                                                                              DatabaseDescriptor.getStoragePort());
            }
        }
        return broadcastInetAddressAndPort;
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getStreamingConnectionsPerHost(),streaming_connections_per_host,(M)org.apache.cassandra.dht.RangeStreamer:addSourceFilter(org.apache.cassandra.dht.RangeStreamer$SourceFilter),addSourceFilter,RangeStreamer,../data/xml/cassandra_call_methods/RangeStreamer.xml,"
public void addSourceFilter(SourceFilter filter)
    {
        sourceFilters.add(filter);
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getStreamingConnectionsPerHost(),streaming_connections_per_host,(M)org.apache.cassandra.service.StorageService:getTokenFactory(),getTokenFactory,StorageService,../data/xml/cassandra_call_methods/StorageService.xml,"
public TokenFactory getTokenFactory()
    {
        return tokenMetadata.partitioner.getTokenFactory();
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getStreamingConnectionsPerHost(),streaming_connections_per_host,"(O)org.apache.cassandra.dht.Range:<init>(org.apache.cassandra.dht.RingPosition,org.apache.cassandra.dht.RingPosition)",<init>,Range,../data/xml/cassandra_call_methods/Range.xml,"
public Range(T left, T right)
    {
        super(left, right);
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getStreamingConnectionsPerHost(),streaming_connections_per_host,"(O)org.apache.cassandra.locator.RangesAtEndpoint$Builder:<init>(org.apache.cassandra.locator.InetAddressAndPort,int)",<init>,RangesAtEndpoint$Builder,../data/xml/cassandra_call_methods/RangesAtEndpoint.xml,"/**
 * A ReplicaCollection for Ranges occurring at an endpoint. All Replica will be for the same endpoint,
 * and must be unique Ranges (though overlapping ranges are presently permitted, these should probably not be permitted to occur)
 */
public class RangesAtEndpoint extends AbstractReplicaCollection<RangesAtEndpoint>
{
    private static ReplicaMap<Range<Token>> rangeMap(ReplicaList list) { return new ReplicaMap<>(list, Replica::range); }
    private static final ReplicaMap<Range<Token>> EMPTY_MAP = rangeMap(EMPTY_LIST);

    private final InetAddressAndPort endpoint;

    // volatile not needed, as all of these caching collections have final members,
    // besides (transitively) those that cache objects that themselves have only final members
    private ReplicaMap<Range<Token>> byRange;
    private RangesAtEndpoint onlyFull;
    private RangesAtEndpoint onlyTransient;

    private RangesAtEndpoint(InetAddressAndPort endpoint, ReplicaList list, ReplicaMap<Range<Token>> byRange)
    {
        super(list);
        this.endpoint = endpoint;
        this.byRange = byRange;
        assert endpoint != null;
    }

    public InetAddressAndPort endpoint()
    {
        return endpoint;
    }

    @Override
    public Set<InetAddressAndPort> endpoints()
    {
        return Collections.unmodifiableSet(list.isEmpty()
                ? Collections.emptySet()
                : Collections.singleton(endpoint)
        );
    }

    /**
     * @return a set of all unique Ranges
     * This method is threadsafe, though it is not synchronised
     */
    public Set<Range<Token>> ranges()
    {
        return byRange().keySet();
    }

    /**
     * @return a map of all Ranges, to their owning Replica instance
     * This method is threadsafe, though it is not synchronised
     */
    public Map<Range<Token>, Replica> byRange()
    {
        ReplicaMap<Range<Token>> map = byRange;
        if (map == null)
            byRange = map = rangeMap(list);
        return map;
    }

    @Override
    protected RangesAtEndpoint snapshot(ReplicaList newList)
    {
        if (newList.isEmpty()) return empty(endpoint);
        ReplicaMap<Range<Token>> byRange = null;
        if (this.byRange != null && list.isSubList(newList))
            byRange = this.byRange.forSubList(newList);
        return new RangesAtEndpoint(endpoint, newList, byRange);
    }

    @Override
    public RangesAtEndpoint snapshot()
    {
        return this;
    }

    @Override
    public ReplicaCollection.Builder<RangesAtEndpoint> newBuilder(int initialCapacity)
    {
        return new Builder(endpoint, initialCapacity);
    }

    @Override
    public boolean contains(Replica replica)
    {
        return replica != null
                && Objects.equals(
                        byRange().get(replica.range()),
                        replica);
    }

    public RangesAtEndpoint onlyFull()
    {
        RangesAtEndpoint result = onlyFull;
        if (result == null)
            onlyFull = result = filter(Replica::isFull);
        return result;
    }

    public RangesAtEndpoint onlyTransient()
    {
        RangesAtEndpoint result = onlyTransient;
        if (result == null)
            onlyTransient = result = filter(Replica::isTransient);
        return result;
    }

    public boolean contains(Range<Token> range, boolean isFull)
    {
        Replica replica = byRange().get(range);
        return replica != null && replica.isFull() == isFull;
    }

    /**
     * @return if there are no wrap around ranges contained in this RangesAtEndpoint, return self;
     * otherwise, return a RangesAtEndpoint covering the same logical portions of the ring, but with those ranges unwrapped
     */
    public RangesAtEndpoint unwrap()
    {
        int wrapAroundCount = 0;
        for (Replica replica : this)
        {
            if (replica.range().isWrapAround())
                ++wrapAroundCount;
        }

        assert wrapAroundCount <= 1;
        if (wrapAroundCount == 0)
            return snapshot();

        RangesAtEndpoint.Builder builder = builder(endpoint, size() + wrapAroundCount);
        for (Replica replica : this)
        {
            if (!replica.range().isWrapAround())
            {
                builder.add(replica);
                continue;
            }
            for (Range<Token> range : replica.range().unwrap())
                builder.add(replica.decorateSubrange(range));
        }
        return builder.build();
    }

    public static Collector<Replica, Builder, RangesAtEndpoint> collector(InetAddressAndPort endpoint)
    {
        return collector(ImmutableSet.of(), () -> new Builder(endpoint));
    }

    public static class Builder extends RangesAtEndpoint implements ReplicaCollection.Builder<RangesAtEndpoint>
    {
        boolean built;
        public Builder(InetAddressAndPort endpoint) { this(endpoint, 0); }
        public Builder(InetAddressAndPort endpoint, int capacity) { this(endpoint, new ReplicaList(capacity)); }
        private Builder(InetAddressAndPort endpoint, ReplicaList list) { super(endpoint, list, rangeMap(list)); }

        public RangesAtEndpoint.Builder add(Replica replica)
        {
            return add(replica, Conflict.DUPLICATE);
        }

        public RangesAtEndpoint.Builder add(Replica replica, Conflict ignoreConflict)
        {
            if (built) throw new IllegalStateException();
            Preconditions.checkNotNull(replica);
            if (!Objects.equals(super.endpoint, replica.endpoint()))
                throw new IllegalArgumentException(""Replica "" + replica + "" has incorrect endpoint (expected "" + super.endpoint + "")"");

            if (!super.byRange.internalPutIfAbsent(replica, list.size()))
            {
                switch (ignoreConflict)
                {
                    case DUPLICATE:
                        if (byRange().get(replica.range()).equals(replica))
                            break;
                    case NONE:
                        throw new IllegalArgumentException(""Conflicting replica added (expected unique ranges): ""
                                + replica + ""; existing: "" + byRange().get(replica.range()));
                    case ALL:
                }
                return this;
            }

            list.add(replica);
            return this;
        }

        @Override
        public RangesAtEndpoint snapshot()
        {
            return snapshot(list.subList(0, list.size()));
        }

        public RangesAtEndpoint build()
        {
            built = true;
            return new RangesAtEndpoint(super.endpoint, super.list, super.byRange);
        }
    }

    public static Builder builder(InetAddressAndPort endpoint)
    {
        return new Builder(endpoint);
    }
    public static Builder builder(InetAddressAndPort endpoint, int capacity)
    {
        return new Builder(endpoint, capacity);
    }

    public static RangesAtEndpoint empty(InetAddressAndPort endpoint)
    {
        return new RangesAtEndpoint(endpoint, EMPTY_LIST, EMPTY_MAP);
    }

    public static RangesAtEndpoint of(Replica replica)
    {
        ReplicaList one = new ReplicaList(1);
        one.add(replica);
        return new RangesAtEndpoint(replica.endpoint(), one, rangeMap(one));
    }

    public static RangesAtEndpoint of(Replica ... replicas)
    {
        return copyOf(Arrays.asList(replicas));
    }

    public static RangesAtEndpoint copyOf(List<Replica> replicas)
    {
        if (replicas.isEmpty())
            throw new IllegalArgumentException(""Must specify a non-empty collection of replicas"");
        return builder(replicas.get(0).endpoint(), replicas.size()).addAll(replicas).build();
    }


    /**
     * Use of this method to synthesize Replicas is almost always wrong. In repair it turns out the concerns of transient
     * vs non-transient are handled at a higher level, but eventually repair needs to ask streaming to actually move
     * the data and at that point it doesn't have a great handle on what the replicas are and it doesn't really matter.
     *
     * Streaming expects to be given Replicas with each replica indicating what type of data (transient or not transient)
     * should be sent.
     *
     * So in this one instance we can lie to streaming and pretend all the replicas are full and use a dummy address
     * and it doesn't matter because streaming doesn't rely on the address for anything other than debugging and full
     * is a valid value for transientness because streaming is selecting candidate tables from the repair/unrepaired
     * set already.
     * @param ranges
     * @return
     */
    @VisibleForTesting
    public static RangesAtEndpoint toDummyList(Collection<Range<Token>> ranges)
    {
        InetAddressAndPort dummy;
        try
        {
            dummy = InetAddressAndPort.getByNameOverrideDefaults(""0.0.0.0"", 0);
        }
        catch (UnknownHostException e)
        {
            throw new RuntimeException(e);
        }

        //For repair we are less concerned with full vs transient since repair is already dealing with those concerns.
        //Always say full and then if the repair is incremental or not will determine what is streamed.
        return ranges.stream()
                .map(range -> new Replica(dummy, range, true))
                .collect(collector(dummy));
    }

    public static boolean isDummyList(RangesAtEndpoint ranges)
    {
        return all(ranges, range -> range.endpoint().getHostAddress(true).equals(""0.0.0.0:0""));
    }

    /**
     * @return concatenate two DISJOINT collections together
     */
    public static RangesAtEndpoint concat(RangesAtEndpoint replicas, RangesAtEndpoint extraReplicas)
    {
        return AbstractReplicaCollection.concat(replicas, extraReplicas, NONE);
    }

}


RangesAtEndpoint>

private RangesAtEndpoint 

private RangesAtEndpoint 

private RangesAtEndpoint(InetAddressAndPort endpoint, ReplicaList list, ReplicaMap<Range<Token>> byRange)
    {
        super(list);
        this.endpoint = endpoint;
        this.byRange = byRange;
        assert endpoint != null;
    }

    

protected RangesAtEndpoint 

RangesAtEndpoint(endpoint, newList, byRange)

public RangesAtEndpoint 

RangesAtEndpoint>

public RangesAtEndpoint 

RangesAtEndpoint 

public RangesAtEndpoint 

RangesAtEndpoint 

public RangesAtEndpoint 

RangesAtEndpoint.Builder

RangesAtEndpoint>

RangesAtEndpoint

RangesAtEndpoint>

RangesAtEndpoint.Builder

RangesAtEndpoint.Builder

public RangesAtEndpoint 

public RangesAtEndpoint 

RangesAtEndpoint(super.endpoint, super.list, super.byRange)

public static RangesAtEndpoint 

RangesAtEndpoint(endpoint, EMPTY_LIST, EMPTY_MAP)

public static RangesAtEndpoint 

RangesAtEndpoint(replica.endpoint(), one, rangeMap(one))

public static RangesAtEndpoint 

public static RangesAtEndpoint 

public static RangesAtEndpoint 

RangesAtEndpoint 

public static RangesAtEndpoint 

RangesAtEndpoint 

RangesAtEndpoint "
org.apache.cassandra.config.DatabaseDescriptor:getStreamingConnectionsPerHost(),streaming_connections_per_host,(M)org.apache.cassandra.locator.RangesAtEndpoint:iterator(),iterator,RangesAtEndpoint,../data/xml/cassandra_call_methods/RangesAtEndpoint.xml,not found
org.apache.cassandra.config.DatabaseDescriptor:getStreamingConnectionsPerHost(),streaming_connections_per_host,(M)org.apache.cassandra.locator.Replica:contains(org.apache.cassandra.dht.Range),contains,Replica,../data/xml/cassandra_call_methods/Replica.xml,"
public boolean contains(Range<Token> that)
    {
        return range().contains(that);
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getStreamingConnectionsPerHost(),streaming_connections_per_host,(M)org.apache.cassandra.locator.Replica:decorateSubrange(org.apache.cassandra.dht.Range),decorateSubrange,Replica,../data/xml/cassandra_call_methods/Replica.xml,"
public Replica decorateSubrange(Range<Token> subrange)
    {
        Preconditions.checkArgument(range.contains(subrange));
        return new Replica(endpoint(), subrange, isFull());
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getStreamingConnectionsPerHost(),streaming_connections_per_host,(M)org.apache.cassandra.locator.RangesAtEndpoint$Builder:add(org.apache.cassandra.locator.Replica),add,RangesAtEndpoint$Builder,../data/xml/cassandra_call_methods/RangesAtEndpoint.xml,"
public RangesAtEndpoint.Builder add(Replica replica)
        {
            return add(replica, Conflict.DUPLICATE);
        }

        

public RangesAtEndpoint.Builder add(Replica replica, Conflict ignoreConflict)
        {
            if (built) throw new IllegalStateException();
            Preconditions.checkNotNull(replica);
            if (!Objects.equals(super.endpoint, replica.endpoint()))
                throw new IllegalArgumentException(""Replica "" + replica + "" has incorrect endpoint (expected "" + super.endpoint + "")"");

            if (!super.byRange.internalPutIfAbsent(replica, list.size()))
            {
                switch (ignoreConflict)
                {
                    case DUPLICATE:
                        if (byRange().get(replica.range()).equals(replica))
                            break;
                    case NONE:
                        throw new IllegalArgumentException(""Conflicting replica added (expected unique ranges): ""
                                + replica + ""; existing: "" + byRange().get(replica.range()));
                    case ALL:
                }
                return this;
            }

            list.add(replica);
            return this;
        }

        "
org.apache.cassandra.config.DatabaseDescriptor:getStreamingConnectionsPerHost(),streaming_connections_per_host,(M)org.apache.cassandra.dht.Range:toString(),toString,Range,../data/xml/cassandra_call_methods/Range.xml,"
@Override
    public String toString()
    {
        return ""("" + left + "","" + right + ""]"";
    }

    "
org.apache.cassandra.config.DatabaseDescriptor:getStreamingConnectionsPerHost(),streaming_connections_per_host,(M)org.apache.cassandra.locator.RangesAtEndpoint$Builder:build(),build,RangesAtEndpoint$Builder,../data/xml/cassandra_call_methods/RangesAtEndpoint.xml,"
public RangesAtEndpoint build()
        {
            built = true;
            return new RangesAtEndpoint(super.endpoint, super.list, super.byRange);
        }
    }"
org.apache.cassandra.config.DatabaseDescriptor:getStreamingConnectionsPerHost(),streaming_connections_per_host,(M)org.apache.cassandra.streaming.StreamResultFuture:get(),get,StreamResultFuture,../data/xml/cassandra_call_methods/StreamResultFuture.xml,not found
